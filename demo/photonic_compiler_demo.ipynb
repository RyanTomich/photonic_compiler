{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "\n",
    "parent_path = os.path.dirname(os.getcwd())\n",
    "target_dir = f'{parent_path}/model_to_graph'\n",
    "sys.path.append(target_dir)\n",
    "\n",
    "# import Relay_compiler\n",
    "import graph_visualization as gv\n",
    "import hardware as hw\n",
    "import dijkstra as dijk\n",
    "import stacked_graph as sg\n",
    "import testing as test\n",
    "import data_collection as dc\n",
    "from main import forward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Inference (gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token(model_name, prompt):\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_name, torchscript=True)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    model.tie_weights()\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "\n",
    "\n",
    "    model = model.eval()  # Change to eval mode\n",
    "\n",
    "    # gen_tokens = model.generate(input_ids, do_sample=False, max_new_tokens=10, streamer=streamer)\n",
    "    gen_tokens = model.generate(\n",
    "                                    input_ids,\n",
    "                                    attention_mask=attention_mask,\n",
    "                                    do_sample=False,\n",
    "                                    max_length=15,\n",
    "                                    pad_token_id=tokenizer.pad_token_id,\n",
    "                                    streamer=streamer\n",
    "                                )\n",
    "\n",
    "    gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    return input_ids, gen_tokens, gen_text\n",
    "    return int(gen_tokens[0][-1])\n",
    "\n",
    "model_name = 'gpt2'\n",
    "prompt = \"my favorite music is\"\n",
    "\n",
    "prompt_tokens, gen_tokens, gen_text = generate_token(model_name, prompt)\n",
    "print(f'{\"Prompt Tokens:\":<20} {prompt_tokens}')\n",
    "print(f'{\"Generated Tokens:\":<20} {gen_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the model to TVM Relay IR\n",
    "\n",
    "Precomputed to save time (takes about 1 minuet for gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_onnx, input_ids = transformer_torch_to_onnx(model_name, prompt, save=False)\n",
    "# lib = onnx_to_relay(model_onnx, input_ids, write=True, model_name=model_name, opt_level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize computational Graph\n",
    "Computational graph visualization for one token generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relay_path = \"gpt2_graph.json\"\n",
    "\n",
    "CPU_MAX_CLOCK = 5.0875 * 10**9\n",
    "CPU_AVERAGE_CLOCK = 3.208 * 10**9\n",
    "PHU_MIN_CLOCK = 10 * 10**9 #100**9, 10 Ghz\n",
    "\n",
    "hardware = []\n",
    "hw.Hardware._hardware_reset()\n",
    "# hardware.append(hw.CPU(CPU_MAX_CLOCK, 1))\n",
    "hardware.append(hw.CPU(CPU_AVERAGE_CLOCK, 1))\n",
    "# hardware.append(hw.PHU(PHU_MIN_CLOCK, 1, 20))\n",
    "\n",
    "available_hardware = hw.initilize_hardware(hardware)\n",
    "\n",
    "optimization = 'time'\n",
    "# optimization = 'energy'\n",
    "\n",
    "with open(relay_path, encoding=\"utf-8\") as json_file:\n",
    "    raw_json = json.load(json_file)  # returns json file as dict\n",
    "\n",
    "WEIGHT_VARIABLE = optimization\n",
    "graph = sg.StackGraph(raw_json=raw_json, weight_variable=WEIGHT_VARIABLE)\n",
    "gv.adj_to_graph(graph, ax=1, layout='kk', title=f'Subgraph', stack=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizations\n",
    "1) Subdevide\n",
    "2) Stack\n",
    "3) select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Simulation Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relay_path = \"gpt2_graph.json\"\n",
    "\n",
    "CPU_MAX_CLOCK = 5.0875 * 10**9\n",
    "CPU_AVERAGE_CLOCK = 3.208 * 10**9\n",
    "PHU_MIN_CLOCK = 10 * 10**9 #100**9, 10 Ghz\n",
    "\n",
    "hardware = []\n",
    "hw.Hardware._hardware_reset()\n",
    "# hardware.append(hw.CPU(CPU_MAX_CLOCK, 1))\n",
    "hardware.append(hw.CPU(CPU_AVERAGE_CLOCK, 1))\n",
    "hardware.append(hw.PHU(PHU_MIN_CLOCK, 1, 20))\n",
    "\n",
    "available_hardware = hw.initilize_hardware(hardware)\n",
    "\n",
    "# optimization = 'time'\n",
    "optimization = 'energy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_subgraphs(subgraph, sub = False):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    gv.adj_to_graph(subgraph, ax=ax, layout='kk', title=f'Subgraph')\n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "subgraph_to_plot = 3\n",
    "\n",
    "with open(relay_path, encoding=\"utf-8\") as json_file:\n",
    "    raw_json = json.load(json_file)  # returns json file as dict\n",
    "\n",
    "WEIGHT_VARIABLE = optimization\n",
    "graph = sg.StackGraph(raw_json=raw_json, weight_variable=WEIGHT_VARIABLE)\n",
    "stacked_subgraphs = list(dijk.graph_partition(graph))\n",
    "flat_subgraphs = dijk.select_nodes(\n",
    "    stacked_subgraphs, weight_variable=WEIGHT_VARIABLE, config=None\n",
    ")\n",
    "expanded_flat_subgraphs = dijk.expand_nodes(flat_subgraphs)\n",
    "\n",
    "print(f' 3 / {len(flat_subgraphs)}')\n",
    "\n",
    "\n",
    "draw_subgraphs(flat_subgraphs[subgraph_to_plot])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAC's in GP2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduled_flat_graph, end_time, break_points = dijk.schdeule_nodes(\n",
    "    graph, expanded_flat_subgraphs, available_hardware\n",
    ")\n",
    "time_profile = dc.get_time_profile(scheduled_flat_graph)\n",
    "print(time_profile)\n",
    "\n",
    "time_profile_sorted = dict(sorted(time_profile.items(), key=lambda item: item[1]))\n",
    "\n",
    "\n",
    "labels = list(time_profile_sorted.keys())\n",
    "values = list(time_profile_sorted.values())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(labels, values)\n",
    "\n",
    "ax.set_xlabel('Operations')\n",
    "ax.set_ylabel('Time')\n",
    "ax.set_title('Time spend on each type of operation')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2e}', va='bottom', ha='center', fontsize=9)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "forward(relay_path, optimization, available_hardware, profiles = False, get_step_times=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schedule",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
