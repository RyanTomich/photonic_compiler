{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "total = [0]\n",
    "def my_matmult(m1, m2):\n",
    "    ans_mat = np.empty((0,m2.shape[1]))\n",
    "    for row in m1:\n",
    "        ans_row = np.array([])\n",
    "        for col in m2.T:\n",
    "            ans_row = np.append(ans_row, np.dot(row, col))\n",
    "            total[0] += len(row)*2\n",
    "        ans_mat = np.vstack((ans_mat,ans_row))\n",
    "    return ans_mat\n",
    "\n",
    "a = np.random.rand(3, 2)\n",
    "b = np.random.rand(2, 3)\n",
    "\n",
    "print(np.array_equal(a@b, my_matmult(a,b)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def tensor_product(m1, m2):\n",
    "  ans = np.empty( (0, m1.shape[-2], m2.shape[-1]) )\n",
    "  for i in range(m1.shape[0]):\n",
    "    matmult = np.expand_dims(my_matmult(m1[i], m2[i]), axis=0)\n",
    "    ans = np.vstack((ans, matmult))\n",
    "  return ans\n",
    "\n",
    "a = np.random.rand(2, 3, 4)\n",
    "b = np.random.rand(2, 4, 2)\n",
    "\n",
    "print(np.allclose(tensor_product(a,b), a @ b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[24576]\n"
     ]
    }
   ],
   "source": [
    "def nd_tensor_product(m1, m2):\n",
    "  ans_shape = m1.shape[:-2] + (m1.shape[-2], m2.shape[-1])\n",
    "  ans = np.empty(ans_shape)\n",
    "  for i in range(m1.shape[0]):\n",
    "    if len(m1.shape) == 3:\n",
    "        ans[i] = my_matmult(m1[i], m2[i])\n",
    "\n",
    "    else:\n",
    "        ans[i] = nd_tensor_product(m1[i], m2[i])\n",
    "  return ans\n",
    "\n",
    "a = np.random.rand(12, 4, 64)\n",
    "b = np.random.rand(12, 64, 4)\n",
    "\n",
    "total = [0]\n",
    "\n",
    "print(np.allclose(nd_tensor_product(a,b), a @ b))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matix_absmax_quantization(m1, int_type=np.int8):\n",
    "    data_max = np.iinfo(int_type).max\n",
    "    norm_range = math.floor((data_max)**(1/2)/ max(m1.shape))\n",
    "    quantification_constant = norm_range/np.max(abs(m1))\n",
    "    new_type = (m1 * quantification_constant).astype(int_type)\n",
    "    assert np.all((new_type >= -1*norm_range) & (new_type <= norm_range))\n",
    "\n",
    "    return new_type, quantification_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_quant_matmul(m1, m2):\n",
    "    m1, m1_const = matix_absmax_quantization(m1, qualtization_type)\n",
    "    m2, m2_const = matix_absmax_quantization(m2, qualtization_type)\n",
    "\n",
    "    ans = np.matmul(m1,m2)\n",
    "\n",
    "    ans = (ans.astype(np.float64))/ (m1_const * m2_const)\n",
    "    return ans\n",
    "\n",
    "def vector_quant_matmul(m1, m2):\n",
    "    matrix = np.empty((m1.shape[0], m2.shape[-1]))\n",
    "    for row_num in range(len(m1)):\n",
    "        for col_num in range(m2.shape[-1]):\n",
    "            row = m1[row_num]\n",
    "            col = m2[:, col_num]\n",
    "            int_row, row_const = matix_absmax_quantization(row, qualtization_type)\n",
    "            int_col, col_const = matix_absmax_quantization(col, qualtization_type)\n",
    "            int_dot = np.dot(int_row, int_col)\n",
    "            float_dot = int_dot.astype(np.float64)* 1/(row_const * col_const)\n",
    "            if np.isnan(float_dot):\n",
    "                float_dot = 0\n",
    "            matrix[row_num][col_num] = float_dot\n",
    "    return matrix\n",
    "\n",
    "def nd_tensor_product(m1, m2, matrix=True):\n",
    "    ans_shape = m1.shape[:-2] + (m1.shape[-2], m2.shape[-1])\n",
    "    ans = np.empty(ans_shape)\n",
    "    if len(ans_shape) < 3:\n",
    "        return matrix_quant_matmul(m1, m2) if matrix else vector_quant_matmul(m1, m2)\n",
    "    else:\n",
    "        for i in range(ans_shape[0]):\n",
    "            ans[i] = nd_tensor_product(m1[i], m2[i], matrix=matrix)\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% vec better 0.882\n",
      "mtx loss 8.205929315715846e-08\n",
      "vec loss 2.285062211066629e-08\n"
     ]
    }
   ],
   "source": [
    "vec_smaller = 0\n",
    "iterations = 1000\n",
    "qualtization_type = np.int64\n",
    "\n",
    "matrix_diff = 0\n",
    "vec_diff = 0\n",
    "\n",
    "\n",
    "for _ in range(iterations):\n",
    "    a = np.random.uniform(-20, 20, size=(2, 10, 4))\n",
    "    b = np.random.uniform(-20, 20, size=(2, 4, 10))\n",
    "\n",
    "    actual = a @ b\n",
    "\n",
    "    mx_quantized = nd_tensor_product(a,b, matrix = True)\n",
    "    # print(f\"{mx_quantized=}\")\n",
    "    mx_loss = np.abs(np.mean(mx_quantized - actual))\n",
    "    matrix_diff += mx_loss\n",
    "    # print(f'matrix loss: {mx_loss}')\n",
    "\n",
    "    vec_quantized = nd_tensor_product(a,b, matrix = False)\n",
    "    # print(f\"{vec_quantized=}\")\n",
    "    vec_loss = np.abs(np.mean(vec_quantized - actual))\n",
    "    # print(vec_loss)\n",
    "    vec_diff += vec_loss\n",
    "    # print(f'vec loss: {vec_loss}')\n",
    "    if vec_loss < mx_loss:\n",
    "        vec_smaller += 1\n",
    "\n",
    "\n",
    "print(f'% vec better {vec_smaller/iterations}')\n",
    "print(f'mtx loss {matrix_diff/iterations}')\n",
    "print(f'vec loss {vec_diff/iterations}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeropoint Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.81947852  16.85851024 -16.94883347  -9.36529621   1.06620353\n",
      " -13.45042167   2.14239435 -17.07961161  -5.89922935  -1.76337706]\n",
      "<class 'numpy.int8'>\n",
      "[ -3.85962954  16.90251559 -16.90251559  -9.31634717   1.06472539\n",
      " -13.44215807   2.12945078 -17.03560626  -5.85598965  -1.73017876]\n"
     ]
    }
   ],
   "source": [
    "def zeropoint_quantization(x, data_type):\n",
    "    data_range = np.iinfo(data_type).max- np.iinfo(data_type).min\n",
    "    q_c = data_range / (np.max(x)- np. min(x))\n",
    "    z_p = -1* np.round(q_c * np.min(x)) - 128\n",
    "    type_x = np.round(q_c * x + z_p).astype(data_type)\n",
    "    return ((type_x - z_p )/ q_c).astype(np.float64)\n",
    "\n",
    "\n",
    "a = np.random.uniform(-20, 20, size=(10))\n",
    "print(a)\n",
    "print(zeropoint_quantization(a, np.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeropoint_quantization(x, data_type):\n",
    "    data_range = np.iinfo(data_type).max- np.iinfo(data_type).min\n",
    "    q_c = data_range / (np.max(x)- np. min(x))\n",
    "    z_p = -1* np.round(q_c * np.min(x)) + np.iinfo(data_type).min\n",
    "    type_x = np.round(q_c * x + z_p).astype(data_type)\n",
    "    return type_x, q_c, z_p\n",
    "\n",
    "\n",
    "# a = np.random.uniform(-20, 20, size=(10))\n",
    "# print(a)\n",
    "# print(zeropoint_quantization(a, np.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_quant_matmul(m1, m2):\n",
    "    matrix = np.empty((m1.shape[0], m2.shape[-1]))\n",
    "    for row_num in range(len(m1)):\n",
    "        for col_num in range(m2.shape[-1]):\n",
    "            row = m1[row_num]\n",
    "            col = m2[:, col_num]\n",
    "            int_row, row_const, row_zero = zeropoint_quantization(row, qualtization_type)\n",
    "            int_col, col_const, col_zero = zeropoint_quantization(col, qualtization_type)\n",
    "            int_dot = np.dot(int_row - row_zero, int_col- col_zero)\n",
    "            float_dot = (int_dot /(row_const * col_const)).astype(np.float64)\n",
    "            if np.isnan(float_dot):\n",
    "                float_dot = 0\n",
    "            matrix[row_num][col_num] = float_dot\n",
    "    return matrix\n",
    "\n",
    "def nd_tensor_product(m1, m2, matrix=True):\n",
    "    ans_shape = m1.shape[:-2] + (m1.shape[-2], m2.shape[-1])\n",
    "    ans = np.empty(ans_shape)\n",
    "    if len(ans_shape) < 3:\n",
    "        return matrix_quant_matmul(m1, m2) if matrix else vector_quant_matmul(m1, m2)\n",
    "    else:\n",
    "        for i in range(ans_shape[0]):\n",
    "            ans[i] = nd_tensor_product(m1[i], m2[i], matrix=matrix)\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-115.50278182  -46.11525114   35.05039982 -483.37567864]\n",
      " [   6.81018286  256.69913419 -450.89811225   58.91757364]]\n",
      "[[-115.50278179  -46.11525112   35.05039982 -483.37567866]\n",
      " [   6.8101829   256.69913423 -450.89811231   58.91757358]]\n"
     ]
    }
   ],
   "source": [
    "qualtization_type = np.int32\n",
    "a = np.random.uniform(-20, 20, size=(2,3))\n",
    "b = np.random.uniform(-20, 20, size=(3,4))\n",
    "\n",
    "actual = a @ b\n",
    "\n",
    "vec_quantized = nd_tensor_product(a,b, matrix = False)\n",
    "\n",
    "print(actual)\n",
    "print(vec_quantized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeropoint or absmax Quantization together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matix_absmax_quantization(m1, int_type=np.int8):\n",
    "    data_max = np.iinfo(int_type).max\n",
    "    norm_range = math.floor((data_max)**(1/2)/ max(m1.shape))\n",
    "    quantification_constant = norm_range/np.max(abs(m1))\n",
    "    new_type = (m1 * quantification_constant).astype(int_type)\n",
    "    assert np.all((new_type >= -1*norm_range) & (new_type <= norm_range))\n",
    "    return new_type, quantification_constant\n",
    "\n",
    "def zeropoint_quantization(x, data_type):\n",
    "    data_range = np.iinfo(data_type).max- np.iinfo(data_type).min\n",
    "    q_c = data_range / (np.max(x)- np. min(x))\n",
    "    z_p = -1* np.round(q_c * np.min(x)) + np.iinfo(data_type).min\n",
    "    type_x = np.round(q_c * x + z_p).astype(data_type)\n",
    "    return type_x, q_c, z_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_quant_matmul(m1, m2, quantization_method):\n",
    "    matrix = np.empty((m1.shape[0], m2.shape[-1]))\n",
    "    for row_num in range(len(m1)):\n",
    "        for col_num in range(m2.shape[-1]):\n",
    "            row = m1[row_num]\n",
    "            col = m2[:, col_num]\n",
    "            if quantization_method == 'absmax':\n",
    "                int_row, row_const = matix_absmax_quantization(row, qualtization_type)\n",
    "                int_col, col_const = matix_absmax_quantization(col, qualtization_type)\n",
    "                int_dot = np.dot(int_row, int_col)\n",
    "                float_dot = int_dot.astype(np.float64)* 1/(row_const * col_const)\n",
    "            elif quantization_method == 'zeropoint':\n",
    "                int_row, row_const, row_zero = zeropoint_quantization(row, qualtization_type)\n",
    "                int_col, col_const, col_zero = zeropoint_quantization(col, qualtization_type)\n",
    "                int_dot = np.dot(int_row - row_zero, int_col- col_zero) # TODO Still multiplying in float64\n",
    "                float_dot = (int_dot /(row_const * col_const)).astype(np.float64)\n",
    "            if np.isnan(float_dot):\n",
    "                float_dot = 0\n",
    "            matrix[row_num][col_num] = float_dot\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def nd_tensor_product(m1, m2, quantization_method = 'absmax'):\n",
    "    ans_shape = m1.shape[:-2] + (m1.shape[-2], m2.shape[-1])\n",
    "    ans = np.empty(ans_shape)\n",
    "    if len(ans_shape) < 3:\n",
    "        return vector_quant_matmul(m1, m2, quantization_method)\n",
    "    else:\n",
    "        for i in range(ans_shape[0]):\n",
    "            ans[i] = nd_tensor_product(m1[i], m2[i], quantization_method=quantization_method)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% vec better 0.997\n",
      "abs loss 7.695771877807612\n",
      "zeropoint loss 0.05291885510640937\n"
     ]
    }
   ],
   "source": [
    "iterations = 1000\n",
    "qualtization_type = np.int8\n",
    "\n",
    "absmax_diff = 0\n",
    "zeropoint_diff = 0\n",
    "\n",
    "zeropoint_smaller = 0\n",
    "for _ in range(iterations):\n",
    "    a = np.random.uniform(-20, 20, size=(2, 10, 4))\n",
    "    b = np.random.uniform(-20, 20, size=(2, 4, 10))\n",
    "\n",
    "    actual = a @ b\n",
    "\n",
    "    absmax_quantized = nd_tensor_product(a,b, quantization_method = 'absmax')\n",
    "    absmax_loss = np.abs(np.mean(absmax_quantized - actual))\n",
    "    absmax_diff += absmax_loss\n",
    "\n",
    "    zeropoint_quantized = nd_tensor_product(a,b, quantization_method = 'zeropoint')\n",
    "    zeropoint_loss = np.abs(np.mean(zeropoint_quantized - actual))\n",
    "    zeropoint_diff += zeropoint_loss\n",
    "    if zeropoint_loss < absmax_loss:\n",
    "        zeropoint_smaller += 1\n",
    "\n",
    "\n",
    "print(f'% vec better {zeropoint_smaller/iterations}')\n",
    "print(f'abs loss {absmax_diff/iterations}')\n",
    "print(f'zeropoint loss {zeropoint_diff/iterations}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight\n",
      "transformer.wpe.weight\n",
      "transformer.h.0.ln_1.weight\n",
      "transformer.h.0.ln_1.bias\n",
      "transformer.h.0.attn.c_attn.weight\n",
      "transformer.h.0.attn.c_attn.bias\n",
      "transformer.h.0.attn.c_proj.weight\n",
      "transformer.h.0.attn.c_proj.bias\n",
      "transformer.h.0.ln_2.weight\n",
      "transformer.h.0.ln_2.bias\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "transformer.h.1.ln_1.weight\n",
      "transformer.h.1.ln_1.bias\n",
      "transformer.h.1.attn.c_attn.weight\n",
      "transformer.h.1.attn.c_attn.bias\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "transformer.h.1.attn.c_proj.bias\n",
      "transformer.h.1.ln_2.weight\n",
      "transformer.h.1.ln_2.bias\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "transformer.h.2.ln_1.weight\n",
      "transformer.h.2.ln_1.bias\n",
      "transformer.h.2.attn.c_attn.weight\n",
      "transformer.h.2.attn.c_attn.bias\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "transformer.h.2.attn.c_proj.bias\n",
      "transformer.h.2.ln_2.weight\n",
      "transformer.h.2.ln_2.bias\n",
      "transformer.h.2.mlp.c_fc.weight\n",
      "transformer.h.2.mlp.c_fc.bias\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "transformer.h.2.mlp.c_proj.bias\n",
      "transformer.h.3.ln_1.weight\n",
      "transformer.h.3.ln_1.bias\n",
      "transformer.h.3.attn.c_attn.weight\n",
      "transformer.h.3.attn.c_attn.bias\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "transformer.h.3.attn.c_proj.bias\n",
      "transformer.h.3.ln_2.weight\n",
      "transformer.h.3.ln_2.bias\n",
      "transformer.h.3.mlp.c_fc.weight\n",
      "transformer.h.3.mlp.c_fc.bias\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "transformer.h.3.mlp.c_proj.bias\n",
      "transformer.h.4.ln_1.weight\n",
      "transformer.h.4.ln_1.bias\n",
      "transformer.h.4.attn.c_attn.weight\n",
      "transformer.h.4.attn.c_attn.bias\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "transformer.h.4.attn.c_proj.bias\n",
      "transformer.h.4.ln_2.weight\n",
      "transformer.h.4.ln_2.bias\n",
      "transformer.h.4.mlp.c_fc.weight\n",
      "transformer.h.4.mlp.c_fc.bias\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "transformer.h.4.mlp.c_proj.bias\n",
      "transformer.h.5.ln_1.weight\n",
      "transformer.h.5.ln_1.bias\n",
      "transformer.h.5.attn.c_attn.weight\n",
      "transformer.h.5.attn.c_attn.bias\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "transformer.h.5.attn.c_proj.bias\n",
      "transformer.h.5.ln_2.weight\n",
      "transformer.h.5.ln_2.bias\n",
      "transformer.h.5.mlp.c_fc.weight\n",
      "transformer.h.5.mlp.c_fc.bias\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "transformer.h.5.mlp.c_proj.bias\n",
      "transformer.h.6.ln_1.weight\n",
      "transformer.h.6.ln_1.bias\n",
      "transformer.h.6.attn.c_attn.weight\n",
      "transformer.h.6.attn.c_attn.bias\n",
      "transformer.h.6.attn.c_proj.weight\n",
      "transformer.h.6.attn.c_proj.bias\n",
      "transformer.h.6.ln_2.weight\n",
      "transformer.h.6.ln_2.bias\n",
      "transformer.h.6.mlp.c_fc.weight\n",
      "transformer.h.6.mlp.c_fc.bias\n",
      "transformer.h.6.mlp.c_proj.weight\n",
      "transformer.h.6.mlp.c_proj.bias\n",
      "transformer.h.7.ln_1.weight\n",
      "transformer.h.7.ln_1.bias\n",
      "transformer.h.7.attn.c_attn.weight\n",
      "transformer.h.7.attn.c_attn.bias\n",
      "transformer.h.7.attn.c_proj.weight\n",
      "transformer.h.7.attn.c_proj.bias\n",
      "transformer.h.7.ln_2.weight\n",
      "transformer.h.7.ln_2.bias\n",
      "transformer.h.7.mlp.c_fc.weight\n",
      "transformer.h.7.mlp.c_fc.bias\n",
      "transformer.h.7.mlp.c_proj.weight\n",
      "transformer.h.7.mlp.c_proj.bias\n",
      "transformer.h.8.ln_1.weight\n",
      "transformer.h.8.ln_1.bias\n",
      "transformer.h.8.attn.c_attn.weight\n",
      "transformer.h.8.attn.c_attn.bias\n",
      "transformer.h.8.attn.c_proj.weight\n",
      "transformer.h.8.attn.c_proj.bias\n",
      "transformer.h.8.ln_2.weight\n",
      "transformer.h.8.ln_2.bias\n",
      "transformer.h.8.mlp.c_fc.weight\n",
      "transformer.h.8.mlp.c_fc.bias\n",
      "transformer.h.8.mlp.c_proj.weight\n",
      "transformer.h.8.mlp.c_proj.bias\n",
      "transformer.h.9.ln_1.weight\n",
      "transformer.h.9.ln_1.bias\n",
      "transformer.h.9.attn.c_attn.weight\n",
      "transformer.h.9.attn.c_attn.bias\n",
      "transformer.h.9.attn.c_proj.weight\n",
      "transformer.h.9.attn.c_proj.bias\n",
      "transformer.h.9.ln_2.weight\n",
      "transformer.h.9.ln_2.bias\n",
      "transformer.h.9.mlp.c_fc.weight\n",
      "transformer.h.9.mlp.c_fc.bias\n",
      "transformer.h.9.mlp.c_proj.weight\n",
      "transformer.h.9.mlp.c_proj.bias\n",
      "transformer.h.10.ln_1.weight\n",
      "transformer.h.10.ln_1.bias\n",
      "transformer.h.10.attn.c_attn.weight\n",
      "transformer.h.10.attn.c_attn.bias\n",
      "transformer.h.10.attn.c_proj.weight\n",
      "transformer.h.10.attn.c_proj.bias\n",
      "transformer.h.10.ln_2.weight\n",
      "transformer.h.10.ln_2.bias\n",
      "transformer.h.10.mlp.c_fc.weight\n",
      "transformer.h.10.mlp.c_fc.bias\n",
      "transformer.h.10.mlp.c_proj.weight\n",
      "transformer.h.10.mlp.c_proj.bias\n",
      "transformer.h.11.ln_1.weight\n",
      "transformer.h.11.ln_1.bias\n",
      "transformer.h.11.attn.c_attn.weight\n",
      "transformer.h.11.attn.c_attn.bias\n",
      "transformer.h.11.attn.c_proj.weight\n",
      "transformer.h.11.attn.c_proj.bias\n",
      "transformer.h.11.ln_2.weight\n",
      "transformer.h.11.ln_2.bias\n",
      "transformer.h.11.mlp.c_fc.weight\n",
      "transformer.h.11.mlp.c_fc.bias\n",
      "transformer.h.11.mlp.c_proj.weight\n",
      "transformer.h.11.mlp.c_proj.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.ln_f.bias\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torchscript=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "parameters = {}\n",
    "for name, val in state_dict.items():\n",
    "    parameters[name] = np.round(val.numpy().astype(np.float64), 4)\n",
    "\n",
    "for name in parameters:\n",
    "    print (name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "5.3206792968749985\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weight = parameters['transformer.h.0.mlp.c_proj.weight']\n",
    "sums = []\n",
    "for i in range(weight.shape[-1]):\n",
    "    sums.append(np.abs(sum(weight[:, i])))\n",
    "# for i in range(weight.shape[0]):\n",
    "#     sums.append(np.abs(sum(weight[i])))\n",
    "\n",
    "print(len(sums))\n",
    "print(np.mean(sums))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(sums)), sums, color='skyblue')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Sums of Weights in GPT-2 Transformer Layer')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Sum of Weights')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.13020833333333331\n"
     ]
    }
   ],
   "source": [
    "average = np.mean(sums)\n",
    "total_over = 0\n",
    "\n",
    "for index, item in enumerate(sums):\n",
    "    if item > average*10:\n",
    "        total_over += 1\n",
    "\n",
    "print(total_over)\n",
    "print (total_over / len(sums)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matix_absmax_quantization(m1, int_type=np.int8):\n",
    "    data_max = np.iinfo(int_type).max\n",
    "    norm_range = math.floor((data_max)**(1/2)/ max(m1.shape))\n",
    "    quantification_constant = norm_range/np.max(abs(m1))\n",
    "    new_type = (m1 * quantification_constant).astype(int_type)\n",
    "    assert np.all((new_type >= -1*norm_range) & (new_type <= norm_range))\n",
    "    return new_type, quantification_constant\n",
    "\n",
    "def zeropoint_quantization(x, data_type):\n",
    "    data_range = np.iinfo(data_type).max- np.iinfo(data_type).min\n",
    "    q_c = data_range / (np.max(x)- np. min(x))\n",
    "    z_p = -1* np.round(q_c * np.min(x)) + np.iinfo(data_type).min\n",
    "    type_x = np.round(q_c * x + z_p).astype(data_type)\n",
    "    return type_x, q_c, z_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_quant_matmul(m1, m2, quantization_method):\n",
    "    matrix = np.empty((m1.shape[0], m2.shape[-1]))\n",
    "    for row_num in range(len(m1)):\n",
    "        for col_num in range(m2.shape[-1]):\n",
    "            row = m1[row_num]\n",
    "            col = m2[:, col_num]\n",
    "            if quantization_method == 'absmax':\n",
    "                int_row, row_const = matix_absmax_quantization(row, qualtization_type)\n",
    "                int_col, col_const = matix_absmax_quantization(col, qualtization_type)\n",
    "                int_dot = np.dot(int_row, int_col)\n",
    "                float_dot = int_dot.astype(np.float64)* 1/(row_const * col_const)\n",
    "            if np.isnan(float_dot):\n",
    "                float_dot = 0\n",
    "            matrix[row_num][col_num] = float_dot\n",
    "    return matrix\n",
    "\n",
    "def separate_outliers(m1, m2, quantization_method):\n",
    "    col_sums = []\n",
    "    for i in range(m1.shape[-1]):\n",
    "        col_sums.append(np.abs(sum(m1[:, i])))\n",
    "    col_sum_mean = np.mean(col_sums)\n",
    "\n",
    "    m1_outliers = np.empty((m1.shape[0], 0))\n",
    "    m2_outliers = np.empty((0 ,m2.shape[-1]))\n",
    "    m1_inliers = np.empty((m1.shape[0], 0))\n",
    "    m2_inliers = np.empty((0 ,m2.shape[-1]))\n",
    "\n",
    "    for index, item in enumerate(col_sums):\n",
    "        if item > col_sum_mean*10:\n",
    "            m1_outliers = np.hstack((m1_outliers, m1[:, index][:, np.newaxis]))\n",
    "            m2_outliers = np.vstack((m2_outliers, m2[index]))\n",
    "        else:\n",
    "            m1_inliers = np.hstack((m1_inliers, m1[:, index][:, np.newaxis]))\n",
    "            m2_inliers = np.vstack((m2_inliers, m2[index]))\n",
    "\n",
    "    print(m1_outliers.shape)\n",
    "\n",
    "    outliers = np.matmul(m1_outliers, m2_outliers)\n",
    "    inliers = vector_quant_matmul(m1_inliers, m2_inliers, quantization_method)\n",
    "\n",
    "    return outliers + inliers\n",
    "\n",
    "\n",
    "def nd_tensor_product(m1, m2, quantization_method = 'absmax'):\n",
    "    ans_shape = m1.shape[:-2] + (m1.shape[-2], m2.shape[-1])\n",
    "    ans = np.empty(ans_shape)\n",
    "    if len(ans_shape) < 3:\n",
    "        return separate_outliers(m1, m2, quantization_method)\n",
    "    else:\n",
    "        for i in range(ans_shape[0]):\n",
    "            ans[i] = nd_tensor_product(m1[i], m2[i], quantization_method=quantization_method)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1644001/873596053.py:11: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  float_dot = int_dot.astype(np.float64)* 1/(row_const * col_const)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 100)\n"
     ]
    }
   ],
   "source": [
    "iterations = 1000\n",
    "qualtization_type = np.int8\n",
    "\n",
    "# absmax_diff = 0\n",
    "# zeropoint_diff = 0\n",
    "\n",
    "# for _ in range(iterations):\n",
    "#     a = np.random.uniform(-20, 20, size=(2, 10, 4))\n",
    "#     b = np.random.uniform(-20, 20, size=(2, 4, 10))\n",
    "\n",
    "#     actual = a @ b\n",
    "\n",
    "#     absmax_quantized = nd_tensor_product(a,b, quantization_method = 'absmax')\n",
    "#     absmax_loss = np.abs(np.mean(absmax_quantized - actual))\n",
    "#     absmax_diff += absmax_loss\n",
    "\n",
    "# print(f'abs loss {absmax_diff/iterations}')\n",
    "\n",
    "a = np.random.uniform(-20, 20, size=(768, 100))\n",
    "absmax_quantized = nd_tensor_product(weight,a, quantization_method = 'absmax')\n",
    "print(absmax_quantized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
