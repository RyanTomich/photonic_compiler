{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using packages\n",
    "\n",
    "Differen gpt2 model options\n",
    "- gpt2: This is the \"small\" version of GPT-2. It has 124 million parameters. 768 context size, 12 decode blocks\n",
    "- gpt2-medium: This is the \"medium\" version of GPT-2. It has 355 million parameters.\n",
    "- gpt2-large: This is the \"large\" version of GPT-2. It has 774 million parameters. 1280 context size 36 decode blocks\n",
    "- gpt2-xl: This is the \"extra large\" version of GPT-2. It has 1.5 billion parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"output_attentions\": true,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained('gpt2', output_attentions=True, activation_function = 'gelu') # loading gpt2 from transformers library\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # loading gpt2 tokenizer from transformers library\n",
    "print(gpt2.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my favorite music is usually found in the early days before the Beatles, which is a good one if it's from the early days prior to the Beatles. I got my first copy, and it was for my first music record. When I discovered the Beatles, I'd always played with someone else, but that was before I'd heard of them.\n",
      "\n",
      "How did that make sense for you?\n",
      "\n",
      "I read an interview with Peter Stradlin on the radio where he said there was an\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/en/model_doc/gpt2\n",
    "prompt = \"my favorite music is\"\n",
    "input_ids = gpt2_tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# print(f'{type(input_ids)}{input_ids}, :{len(input_ids)}')\n",
    "gen_tokens = gpt2.generate(input_ids, do_sample=True, temperature=0.9, max_length=100)\n",
    "# print(f'{type(gen_tokens)} {gen_tokens}, :{len(gen_tokens[0])}')\n",
    "gen_text = gpt2_tokenizer.batch_decode(gen_tokens)[0]\n",
    "\n",
    "# gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token # set the padding token\n",
    "# input_ids = gpt2_tokenizer.encode(input_text, return_tensors='pt') # tokenize input\n",
    "# output = gpt2.generate(input_ids, max_length=max) # run inference\n",
    "# generated_text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True) # decode output tokens\n",
    "print(gen_text)\n",
    "\n",
    "# tok = gpt2_tokenizer.encode(tok, return_tensors='np', padding='max_length', truncation=True, max_length=max_token_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight: torch.Size([50257, 768])\n",
      "transformer.wpe.weight: torch.Size([1024, 768])\n",
      "transformer.ln_f.weight: torch.Size([768])\n",
      "transformer.ln_f.bias: torch.Size([768])\n",
      "lm_head.weight: torch.Size([50257, 768])\n",
      "transformer.h.0.ln_1.weight: (768,)\n",
      "transformer.h.0.ln_1.weight: torch.Size([768])\n",
      "transformer.h.0.ln_1.bias: (768,)\n",
      "transformer.h.0.ln_1.bias: torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.0.attn.c_attn.weight: torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias: (2304,)\n",
      "transformer.h.0.attn.c_attn.bias: torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.0.attn.c_proj.weight: torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias: (768,)\n",
      "transformer.h.0.attn.c_proj.bias: torch.Size([768])\n",
      "transformer.h.0.ln_2.weight: (768,)\n",
      "transformer.h.0.ln_2.weight: torch.Size([768])\n",
      "transformer.h.0.ln_2.bias: (768,)\n",
      "transformer.h.0.ln_2.bias: torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.0.mlp.c_fc.weight: torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.0.mlp.c_fc.bias: torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.0.mlp.c_proj.weight: torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias: (768,)\n",
      "transformer.h.0.mlp.c_proj.bias: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "state_dict = gpt2.state_dict()\n",
    "parameters = {}\n",
    "for name, val in state_dict.items():\n",
    "    parameters[name] = val.numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "for name, param in state_dict.items():\n",
    "    ans = param.numpy()\n",
    "    if 'h.' not in name: # each h.# refers to a transformer blocks\n",
    "        print(f'{name}: {param.shape}')\n",
    "        pass\n",
    "\n",
    "for i in range(36):\n",
    "    counter = 0\n",
    "    for name, param in state_dict.items():\n",
    "        ans = param.numpy()\n",
    "        if 'h.'+ str(i)+ '.' in name and i == 0: # each h.# refers to a transformer block\n",
    "            print(f'{name}: {ans.shape}')\n",
    "            print(f'{name}: {param.shape}')\n",
    "            counter +=1\n",
    "    # print(f'h.{i}: {counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = parameters\n",
    "\n",
    "def torch_to_numpy(tensor): # not nessessarry?\n",
    "    if tensor.is_cuda:\n",
    "        tensor = tensor.cpu()\n",
    "    numpy_array = tensor.numpy()\n",
    "    return numpy_array.copy()\n",
    "\n",
    "def softmax(vec, temperature = None): # the, and,\n",
    "\n",
    "    # vec = torch.tensor(vec, dtype=torch.float32)\n",
    "    # if temperature:\n",
    "    #     vec /= temperature\n",
    "    # return F.softmax(vec, dim=-1).numpy()\n",
    "\n",
    "    max_val = np.max(vec)\n",
    "    if temperature:\n",
    "        exp = np.exp((vec - max_val)/ temperature)\n",
    "    else:\n",
    "         exp = np.exp(vec - max_val)\n",
    "\n",
    "    sum_exp = np.sum(exp)\n",
    "    norm_vec = exp/sum_exp\n",
    "    assert 0.975 < np.sum(norm_vec) < 1.025\n",
    "    return norm_vec\n",
    "\n",
    "def log_softmax(vec, epsilon=1e-05): # puncuation\n",
    "    max_val = np.max(vec)\n",
    "    exp = np.exp(vec - max_val)\n",
    "    log_sum_exp = max_val + np.log(np.sum(exp))\n",
    "    return vec - log_sum_exp\n",
    "\n",
    "def repo_softmax(vec): # error\n",
    "    x = vec - np.argmin(vec)\n",
    "    ex = np.exp(x)\n",
    "    return ex/ np.sum(ex)\n",
    "\n",
    "# activation functions\n",
    "def gelu(x):\n",
    "    # a = torch.nn.functional.gelu(torch.tensor(x)).numpy()\n",
    "    # assert a.shape == x.shape\n",
    "    # return a\n",
    "    # from https://github.com/openai/gpt-2.git\n",
    "    # x(np_array) Gausien error liner unit\n",
    "    # return F.gelu(torch.tensor(x,dtype=torch.float32)).numpy()\n",
    "    return 0.5*x*(1+np.tanh(np.sqrt(2/np.pi)*(x+0.044715*np.power(x, 3))))\n",
    "\n",
    "def ReLU(x):\n",
    "    # x(np_array) clip negitive activation\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Transformer functions\n",
    "def embed(tok):\n",
    "    '''\n",
    "    creat embeding matrix (token, token embeding vector 768)\n",
    "    tok(np_array): 1d array of toek encodings\n",
    "    paramaters(dict): dictionary maping names to tensors\n",
    "    '''\n",
    "    # sequence_length = tok.shape[0]\n",
    "    # position_ids = torch.tensor(np.arange(sequence_length)) #indicies\n",
    "    # tok = torch.tensor(tok)\n",
    "    # return (gpt2.transformer.wte(tok) + gpt2.transformer.wpe(position_ids)).detach().numpy()\n",
    "\n",
    "\n",
    "    # word token embeddings\n",
    "    tok_emb = parameters['transformer.wte.weight'][tok,:]\n",
    "\n",
    "    # word position embeddings\n",
    "    sequence_length = tok.shape[0]\n",
    "    position_ids = np.arange(sequence_length) #indicies\n",
    "    position_emb = parameters['transformer.wpe.weight'][position_ids,:]\n",
    "    assert tok_emb.shape == position_emb.shape\n",
    "    return tok_emb + position_emb\n",
    "\n",
    "def li_norm(x, gamma, beta, epsilon=1e-5):\n",
    "    '''\n",
    "    layer batch normalization\n",
    "    x(np_array): array to normalize\n",
    "    gamma(np_array): scailing paramater vector\n",
    "    beta(np_array): offset paramater vector\n",
    "    epsilon(float): div_0_error prevention\n",
    "    '''\n",
    "    # x = torch.tensor(x, dtype=torch.float32)\n",
    "    # gamma = torch.tensor(gamma, dtype=torch.float32)\n",
    "    # beta = torch.tensor(beta, dtype=torch.float32)\n",
    "    # return (F.layer_norm(x, (x.size(-1),), gamma, beta, eps=epsilon)).numpy()\n",
    "\n",
    "    u = np.mean(x, axis=-1, keepdims=True)\n",
    "    # s = np.var(x, axis=-1, keepdims=True)\n",
    "    s = np.mean(np.square(x-u))\n",
    "    x = (x - u) / np.sqrt(s + epsilon)\n",
    "    return x*gamma + beta\n",
    "\n",
    "def get_head_weights(head_tot, weights, bias):\n",
    "    '''\n",
    "    head_tot(int)\n",
    "    weights(np.matrix) (tok)\n",
    "    bias(np.vec)\n",
    "    return ittorator (tup): head_w, head_b\n",
    "    '''\n",
    "    head_width = int(weights.shape[1] / head_tot)\n",
    "    start = 0\n",
    "    end = head_width\n",
    "    for _ in range(head_tot):\n",
    "        yield (weights[:, start:end], bias[start:end])\n",
    "        start += head_width\n",
    "        end += head_width\n",
    "\n",
    "def split_head(head_param):\n",
    "    '''\n",
    "    head_param (tup): head_w, head_b\n",
    "    '''\n",
    "    weights, bias = head_param\n",
    "    width = int(head_param[1].shape[0] / 3)\n",
    "\n",
    "    start = 0\n",
    "    end = width\n",
    "    for _ in range(3):\n",
    "        yield (weights[:, start:end], bias[start:end])\n",
    "        start += width\n",
    "        end += width\n",
    "\n",
    "def self_attn(emb, block_num, attn_heads = 12):\n",
    "    '''\n",
    "    attention block. 12 heads per block\n",
    "    emb(np_matrix): (tokens, Embedding Size 768)\n",
    "    paramaters(dict): dictionary maping names to tensors\n",
    "    block_num: current head\n",
    "    '''\n",
    "\n",
    "    # projection\n",
    "    weights = parameters['transformer.h.'+ str(block_num) + '.attn.c_proj.weight']\n",
    "    bias = parameters['transformer.h.'+ str(block_num) + '.attn.c_proj.bias']\n",
    "    emb = (emb @ weights) + bias\n",
    "\n",
    "    attn_weights = parameters['transformer.h.'+ str(block_num) + '.attn.c_attn.weight']\n",
    "    attn_bias = parameters['transformer.h.'+ str(block_num) + '.attn.c_attn.bias']\n",
    "\n",
    "    context_matrix = np.empty((emb.shape[0],0))\n",
    "    for head_num, head_param in enumerate(get_head_weights(attn_heads, attn_weights, attn_bias)):\n",
    "        QKV_gen = split_head(head_param)\n",
    "        query_w, query_b = next(QKV_gen)\n",
    "        key_w, key_b = next(QKV_gen)\n",
    "        value_w, value_b = next(QKV_gen)\n",
    "\n",
    "        Q = np.apply_along_axis(lambda x: x + query_b, axis = 1, arr=emb @ query_w)\n",
    "        K = np.apply_along_axis(lambda x: x + key_b, axis = 1, arr=emb @ key_w)\n",
    "        V = np.apply_along_axis(lambda x: x + value_b, axis = 1, arr=emb @ value_w)\n",
    "\n",
    "        print(f'{Q.shape} @ {K.shape}')\n",
    "        attn_score = Q @ K.T\n",
    "\n",
    "        # future_mask\n",
    "        future_mask = np.full(attn_score.shape, 0.0)\n",
    "        future_mask[np.triu_indices_from(future_mask, k=1)] = float('-inf')\n",
    "\n",
    "        attn_score_mask = attn_score + future_mask\n",
    "\n",
    "        attn_score_norm = np.apply_along_axis(lambda x: softmax(x, temperature = 1.2), axis=1, arr=attn_score_mask) # (1024, 1024)\n",
    "\n",
    "        #liner layer?\n",
    "\n",
    "        context_matrix = np.hstack([context_matrix, attn_score_norm @ V])\n",
    "\n",
    "    assert emb.shape == context_matrix.shape\n",
    "    return context_matrix\n",
    "\n",
    "def mlp(emb, block_num):\n",
    "    '''\n",
    "    2 layer multi layer perceptron with gelu activation\n",
    "    emb(np_matrix): (tokens, Embedding Size 768)\n",
    "    paramaters(dict): dictionary maping names to tensors\n",
    "    block_num: current head\n",
    "    '''\n",
    "    weights = parameters['transformer.h.'+ str(block_num) + '.mlp.c_fc.weight']\n",
    "    bias = parameters['transformer.h.'+ str(block_num) + '.mlp.c_fc.bias']\n",
    "    embl1 = (emb @ weights) + bias\n",
    "\n",
    "    embl1 = gelu(embl1)\n",
    "\n",
    "    weights = parameters['transformer.h.'+ str(block_num) + '.mlp.c_proj.weight']\n",
    "    bias = parameters['transformer.h.'+ str(block_num) + '.mlp.c_proj.bias']\n",
    "    return (embl1 @ weights) + bias\n",
    "\n",
    "def top_k(k, vec):\n",
    "    largest = heapq.nlargest(k, range(len(vec)), vec.take)\n",
    "    # print(gpt2_tokenizer.decode(largest, skip_special_tokens=True)) # see words its picking from.\n",
    "    probs = [vec[i] for i in largest]\n",
    "    probs = probs / np.sum(probs) # normalize after the selection\n",
    "    print(np.max(probs))\n",
    "    return random.choices(largest, weights=probs, k=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = parameters\n",
    "\n",
    "def decode_block(emb, block_num):\n",
    "    '''\n",
    "    runs decode block with ln_1 -> attn -> ln_2 -> mlp\n",
    "    emb (np_array): (tokens, Embedding Size 768)\n",
    "    paramaters(dict): dictionary maping names to tensors\n",
    "    block_num: current head\n",
    "    '''\n",
    "\n",
    "    original_emb = copy.deepcopy(emb)\n",
    "\n",
    "    # ln_1 normalization\n",
    "    weights = parameters['transformer.h.'+ str(block_num) + '.ln_1.weight']\n",
    "    bias = parameters['transformer.h.'+ str(block_num) + '.ln_1.bias']\n",
    "    emb_norm1 = li_norm(emb, weights, bias)\n",
    "\n",
    "\n",
    "    context_matrix = self_attn(emb_norm1, block_num)\n",
    "    assert np.array_equal(emb, original_emb)\n",
    "    context_matrix += emb # Residual Connection\n",
    "\n",
    "    # ln_2 normalization\n",
    "    weights = parameters['transformer.h.'+ str(block_num) + '.ln_2.weight']\n",
    "    bias = parameters['transformer.h.'+ str(block_num) + '.ln_2.bias']\n",
    "    emb_norm2 = li_norm(context_matrix, weights, bias, epsilon=1e-5)\n",
    "\n",
    "    emb_mlp = mlp(emb_norm2, block_num)\n",
    "    emb_mlp += emb_norm2    # Residual Connection\n",
    "    return emb_mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_token(tok, transformer_blocks = 12):\n",
    "    '''\n",
    "    Generates the next token in sequence\n",
    "    tok (np_array): 1D token encodigns\n",
    "    parameters(dict): dictionary maping names to tensors\n",
    "    '''\n",
    "    emb = embed(tok) #(tokens, Embedding Size 768)\n",
    "\n",
    "    block_result = copy.deepcopy(emb)\n",
    "    for block in range(transformer_blocks):\n",
    "        block_result = decode_block(block_result, block) # (tokens, Embedding Size 768)\n",
    "\n",
    "    # ln_f\n",
    "    # weights = parameters['transformer.ln_f.weight']\n",
    "    # bias = parameters['transformer.ln_f.bias']\n",
    "    weights = parameters['transformer.ln_f.weight']\n",
    "    bias = parameters['transformer.ln_f.bias']\n",
    "    head_norm = li_norm(block_result, weights, bias)\n",
    "\n",
    "    # lm_head\n",
    "    weights = parameters['lm_head.weight'] # (50257, 768)\n",
    "    logit_matrix = head_norm @ weights.T\n",
    "\n",
    "    # apply softmax to last words logit\n",
    "    last_logit_distrabution = softmax(logit_matrix[-1], temperature = 1.2)\n",
    "    return top_k(40, last_logit_distrabution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight'])\n",
      "<class 'numpy.ndarray'>[27078  2402   257 ... 50256 50256 50256], :1024\n",
      "4\n",
      "[27078  2402   257   640 50256 50256 50256 50256 50256 50256]\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 768) @ (50257, 768)\n",
      "0.6675009602429381\n",
      "[27078  2402   257   640   290 50256 50256 50256 50256 50256]\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 768) @ (50257, 768)\n",
      "0.6606772815048838\n",
      "[27078  2402   257   640   290   290 50256 50256 50256 50256]\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 768) @ (50257, 768)\n",
      "0.6585550775596148\n",
      "[27078  2402   257   640   290   290   290 50256 50256 50256]\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 768) @ (50257, 768)\n",
      "0.65622405758254\n",
      "[27078  2402   257   640   290   290   290   393 50256 50256]\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n",
      "(1024, 64) @ (1024, 64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[314], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     tok \u001b[38;5;241m=\u001b[39m gpt2_tokenizer\u001b[38;5;241m.\u001b[39mdecode(tok, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tok\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43monce upon a time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_generate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[314], line 20\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(prompt, max_token_len, num_generate)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_generate):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tok[:\u001b[38;5;241m10\u001b[39m])\n\u001b[0;32m---> 20\u001b[0m     new_tok \u001b[38;5;241m=\u001b[39m \u001b[43mnext_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     tok[prompt_tok_index] \u001b[38;5;241m=\u001b[39m new_tok\n\u001b[1;32m     22\u001b[0m     prompt_tok_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[313], line 11\u001b[0m, in \u001b[0;36mnext_token\u001b[0;34m(tok, transformer_blocks)\u001b[0m\n\u001b[1;32m      9\u001b[0m block_result \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(emb)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(transformer_blocks):\n\u001b[0;32m---> 11\u001b[0m     block_result \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (tokens, Embedding Size 768)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ln_f\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# weights = parameters['transformer.ln_f.weight']\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# bias = parameters['transformer.ln_f.bias']\u001b[39;00m\n\u001b[1;32m     16\u001b[0m weights \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.ln_f.weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[310], line 19\u001b[0m, in \u001b[0;36mdecode_block\u001b[0;34m(emb, block_num)\u001b[0m\n\u001b[1;32m     15\u001b[0m bias \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(block_num) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.ln_1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m emb_norm1 \u001b[38;5;241m=\u001b[39m li_norm(emb, weights, bias)\n\u001b[0;32m---> 19\u001b[0m context_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb_norm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(emb, original_emb)\n\u001b[1;32m     21\u001b[0m context_matrix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m emb \u001b[38;5;66;03m# Residual Connection\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[309], line 180\u001b[0m, in \u001b[0;36mself_attn\u001b[0;34m(emb, block_num, attn_heads)\u001b[0m\n\u001b[1;32m    176\u001b[0m future_mask[np\u001b[38;5;241m.\u001b[39mtriu_indices_from(future_mask, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    178\u001b[0m attn_score_mask \u001b[38;5;241m=\u001b[39m attn_score \u001b[38;5;241m+\u001b[39m future_mask\n\u001b[0;32m--> 180\u001b[0m attn_score_norm \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_score_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (1024, 1024)\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m#liner layer?\u001b[39;00m\n\u001b[1;32m    184\u001b[0m context_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([context_matrix, attn_score_norm \u001b[38;5;241m@\u001b[39m V])\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/shape_base.py:401\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# save the first result, then compute and save all remaining results\u001b[39;00m\n\u001b[1;32m    400\u001b[0m buff[ind0] \u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m--> 401\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m inds:\n\u001b[1;32m    402\u001b[0m     buff[ind] \u001b[38;5;241m=\u001b[39m asanyarray(func1d(inarr_view[ind], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, matrix):\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# wrap the array, to preserve subclasses\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/shape_base.py:370\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# compute indices for the iteration axes, and append a trailing ellipsis to\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# prevent 0d arrays decaying to scalars, which fixes gh-8642\u001b[39;00m\n\u001b[1;32m    369\u001b[0m inds \u001b[38;5;241m=\u001b[39m ndindex(inarr_view\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 370\u001b[0m inds \u001b[38;5;241m=\u001b[39m (ind \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mEllipsis\u001b[39m,) \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m inds)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# invoke the function on the first item\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/index_tricks.py:703\u001b[0m, in \u001b[0;36mndindex.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    692\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m    Standard iterator method, updates the index and returns the index\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m    tuple.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m \n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m     \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_it\u001b[49m)\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_it\u001b[38;5;241m.\u001b[39mmulti_index\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(prompt, max_token_len = 1024, num_generate = 5):\n",
    "    '''\n",
    "    creates generation feedback loop\n",
    "    prompt(srt)\n",
    "    start_dict(dict): name: paramaters\n",
    "    '''\n",
    "    for name, val in state_dict.items():\n",
    "        parameters[name] = val.numpy().astype(np.float32)\n",
    "\n",
    "    print(parameters.keys())\n",
    "\n",
    "    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "    tok = gpt2_tokenizer.encode(prompt, return_tensors='np', padding='max_length', truncation=True, max_length=max_token_len).squeeze()\n",
    "    print(f'{type(tok)}{tok}, :{len(tok)}')\n",
    "\n",
    "    prompt_tok_index = np.where(tok == gpt2_tokenizer.eos_token_id)[0][0]\n",
    "    print(prompt_tok_index)\n",
    "    for _ in range(num_generate):\n",
    "        print(tok[:10])\n",
    "        new_tok = next_token(tok)\n",
    "        tok[prompt_tok_index] = new_tok\n",
    "        prompt_tok_index += 1\n",
    "\n",
    "\n",
    "    tok = gpt2_tokenizer.decode(tok, skip_special_tokens=True)\n",
    "    return tok\n",
    "\n",
    "print(main('once upon a time', num_generate = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
