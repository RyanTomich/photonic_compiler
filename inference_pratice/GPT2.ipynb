{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using packages\n",
    "\n",
    "Differen gpt2 model options\n",
    "- gpt2: This is the \"small\" version of GPT-2. It has 124 million parameters.\n",
    "- gpt2-medium: This is the \"medium\" version of GPT-2. It has 355 million parameters.\n",
    "- gpt2-large: This is the \"large\" version of GPT-2. It has 774 million parameters.\n",
    "- gpt2-xl: This is the \"extra large\" version of GPT-2. It has 1.5 billion parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained('gpt2') # loading gpt2 from transformers library\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # loading gpt2 tokenizer from transformers library\n",
    "print(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A long time ago in a galaxy far far away...\n",
      "\n",
      "The first human-made planet was discovered in the early 1960s by a team of astronomers from the University of California, Berkeley.\n",
      "\n",
      "The discovery of the first human-made planet was made by a team of astronomers from the University of California, Berkeley.\n",
      "\n",
      "The first human-made planet was discovered in the early 1960s by a team of astronomers from the University of California, Berkeley.\n",
      "\n",
      "The first human-made\n"
     ]
    }
   ],
   "source": [
    "input_text = \"A long time ago in a galaxy far far away ...\"\n",
    "input_ids = gpt2_tokenizer.encode(input_text, return_tensors='pt') # tokenize input\n",
    "output = gpt2.generate(input_ids, max_length=100) # run inference\n",
    "generated_text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True) # decode output tokens\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def torch_to_numpy(tensor):\n",
    "    # nessessarry because numpy will be run on CPU. More complicated otherwise.\n",
    "    if tensor.is_cuda:\n",
    "        tensor = tensor.cpu()\n",
    "    numpy_array = tensor.numpy()\n",
    "    return numpy_array.copy()\n",
    "\n",
    "def softmax(vec):\n",
    "    max_val = np.max(vec)\n",
    "    exp = np.exp(vec - max_val)\n",
    "    sum_exp = np.sum(exp)\n",
    "    return exp/sum_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0.ln_1.weight: (768,)\n",
      "transformer.h.0.ln_1.bias: (768,)\n",
      "transformer.h.0.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.0.attn.c_attn.bias: (2304,)\n",
      "transformer.h.0.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.0.attn.c_proj.bias: (768,)\n",
      "transformer.h.0.ln_2.weight: (768,)\n",
      "transformer.h.0.ln_2.bias: (768,)\n",
      "transformer.h.0.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.0.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.0.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.0.mlp.c_proj.bias: (768,)\n",
      "transformer.h.1.ln_1.weight: (768,)\n",
      "transformer.h.1.ln_1.bias: (768,)\n",
      "transformer.h.1.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.1.attn.c_attn.bias: (2304,)\n",
      "transformer.h.1.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.1.attn.c_proj.bias: (768,)\n",
      "transformer.h.1.ln_2.weight: (768,)\n",
      "transformer.h.1.ln_2.bias: (768,)\n",
      "transformer.h.1.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.1.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.1.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.1.mlp.c_proj.bias: (768,)\n",
      "transformer.h.2.ln_1.weight: (768,)\n",
      "transformer.h.2.ln_1.bias: (768,)\n",
      "transformer.h.2.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.2.attn.c_attn.bias: (2304,)\n",
      "transformer.h.2.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.2.attn.c_proj.bias: (768,)\n",
      "transformer.h.2.ln_2.weight: (768,)\n",
      "transformer.h.2.ln_2.bias: (768,)\n",
      "transformer.h.2.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.2.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.2.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.2.mlp.c_proj.bias: (768,)\n",
      "transformer.h.3.ln_1.weight: (768,)\n",
      "transformer.h.3.ln_1.bias: (768,)\n",
      "transformer.h.3.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.3.attn.c_attn.bias: (2304,)\n",
      "transformer.h.3.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.3.attn.c_proj.bias: (768,)\n",
      "transformer.h.3.ln_2.weight: (768,)\n",
      "transformer.h.3.ln_2.bias: (768,)\n",
      "transformer.h.3.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.3.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.3.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.3.mlp.c_proj.bias: (768,)\n",
      "transformer.h.4.ln_1.weight: (768,)\n",
      "transformer.h.4.ln_1.bias: (768,)\n",
      "transformer.h.4.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.4.attn.c_attn.bias: (2304,)\n",
      "transformer.h.4.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.4.attn.c_proj.bias: (768,)\n",
      "transformer.h.4.ln_2.weight: (768,)\n",
      "transformer.h.4.ln_2.bias: (768,)\n",
      "transformer.h.4.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.4.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.4.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.4.mlp.c_proj.bias: (768,)\n",
      "transformer.h.5.ln_1.weight: (768,)\n",
      "transformer.h.5.ln_1.bias: (768,)\n",
      "transformer.h.5.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.5.attn.c_attn.bias: (2304,)\n",
      "transformer.h.5.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.5.attn.c_proj.bias: (768,)\n",
      "transformer.h.5.ln_2.weight: (768,)\n",
      "transformer.h.5.ln_2.bias: (768,)\n",
      "transformer.h.5.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.5.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.5.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.5.mlp.c_proj.bias: (768,)\n",
      "transformer.h.6.ln_1.weight: (768,)\n",
      "transformer.h.6.ln_1.bias: (768,)\n",
      "transformer.h.6.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.6.attn.c_attn.bias: (2304,)\n",
      "transformer.h.6.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.6.attn.c_proj.bias: (768,)\n",
      "transformer.h.6.ln_2.weight: (768,)\n",
      "transformer.h.6.ln_2.bias: (768,)\n",
      "transformer.h.6.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.6.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.6.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.6.mlp.c_proj.bias: (768,)\n",
      "transformer.h.7.ln_1.weight: (768,)\n",
      "transformer.h.7.ln_1.bias: (768,)\n",
      "transformer.h.7.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.7.attn.c_attn.bias: (2304,)\n",
      "transformer.h.7.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.7.attn.c_proj.bias: (768,)\n",
      "transformer.h.7.ln_2.weight: (768,)\n",
      "transformer.h.7.ln_2.bias: (768,)\n",
      "transformer.h.7.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.7.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.7.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.7.mlp.c_proj.bias: (768,)\n",
      "transformer.h.8.ln_1.weight: (768,)\n",
      "transformer.h.8.ln_1.bias: (768,)\n",
      "transformer.h.8.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.8.attn.c_attn.bias: (2304,)\n",
      "transformer.h.8.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.8.attn.c_proj.bias: (768,)\n",
      "transformer.h.8.ln_2.weight: (768,)\n",
      "transformer.h.8.ln_2.bias: (768,)\n",
      "transformer.h.8.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.8.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.8.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.8.mlp.c_proj.bias: (768,)\n",
      "transformer.h.9.ln_1.weight: (768,)\n",
      "transformer.h.9.ln_1.bias: (768,)\n",
      "transformer.h.9.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.9.attn.c_attn.bias: (2304,)\n",
      "transformer.h.9.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.9.attn.c_proj.bias: (768,)\n",
      "transformer.h.9.ln_2.weight: (768,)\n",
      "transformer.h.9.ln_2.bias: (768,)\n",
      "transformer.h.9.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.9.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.9.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.9.mlp.c_proj.bias: (768,)\n",
      "transformer.h.10.ln_1.weight: (768,)\n",
      "transformer.h.10.ln_1.bias: (768,)\n",
      "transformer.h.10.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.10.attn.c_attn.bias: (2304,)\n",
      "transformer.h.10.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.10.attn.c_proj.bias: (768,)\n",
      "transformer.h.10.ln_2.weight: (768,)\n",
      "transformer.h.10.ln_2.bias: (768,)\n",
      "transformer.h.10.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.10.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.10.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.10.mlp.c_proj.bias: (768,)\n",
      "transformer.h.11.ln_1.weight: (768,)\n",
      "transformer.h.11.ln_1.bias: (768,)\n",
      "transformer.h.11.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.11.attn.c_attn.bias: (2304,)\n",
      "transformer.h.11.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.11.attn.c_proj.bias: (768,)\n",
      "transformer.h.11.ln_2.weight: (768,)\n",
      "transformer.h.11.ln_2.bias: (768,)\n",
      "transformer.h.11.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.11.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.11.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.11.mlp.c_proj.bias: (768,)\n",
      "h.0: 12\n",
      "h.1: 12\n",
      "h.2: 12\n",
      "h.3: 12\n",
      "h.4: 12\n",
      "h.5: 12\n",
      "h.6: 12\n",
      "h.7: 12\n",
      "h.8: 12\n",
      "h.9: 12\n",
      "h.10: 12\n",
      "h.11: 12\n"
     ]
    }
   ],
   "source": [
    "state_dict = gpt2.state_dict()\n",
    "for name, param in state_dict.items():\n",
    "    ans = torch_to_numpy(param)\n",
    "    if 'h.' in name: # each h.# refers to a transformer blocks\n",
    "        print(f'{name}: {ans.shape}')\n",
    "\n",
    "for i in range(12):\n",
    "    counter = 0\n",
    "    for name, param in state_dict.items():\n",
    "        ans = torch_to_numpy(param)\n",
    "        if 'h.'+ str(i)+ '.' in name: # each h.# refers to a transformer block\n",
    "            # print(f'{name}: {ans.shape}')\n",
    "            counter +=1\n",
    "    print(f'h.{i}: {counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_block(emb, parameters, head_num):\n",
    "    '''\n",
    "    emb (): Something...\n",
    "    paramaters(dict): dictionary maping names to tensors\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    transformer.h.0.ln_1.weight: (768,)\n",
    "    transformer.h.0.ln_1.bias: (768,)\n",
    "\n",
    "    transformer.h.0.attn.c_attn.weight: (768, 2304)\n",
    "    transformer.h.0.attn.c_attn.bias: (2304,)\n",
    "    transformer.h.0.attn.c_proj.weight: (768, 768)\n",
    "    transformer.h.0.attn.c_proj.bias: (768,)\n",
    "\n",
    "    transformer.h.0.ln_2.weight: (768,)\n",
    "    transformer.h.0.ln_2.bias: (768,)\n",
    "\n",
    "    transformer.h.0.mlp.c_fc.weight: (768, 3072)\n",
    "    transformer.h.0.mlp.c_fc.bias: (3072,)\n",
    "    transformer.h.0.mlp.c_proj.weight: (3072, 768)\n",
    "    transformer.h.0.mlp.c_proj.bias: (768,)\n",
    "    '''\n",
    "    # ln_1 normalization\n",
    "    weights = parameters['transformer.h.'+ str(head_num) + '.ln_1.weight'].numpy()\n",
    "    bias = parameters['transformer.h.'+ str(head_num) + '.ln_1.bias'].numpy()\n",
    "    emb_norm1 =  (emb * weights) + bias # (768,)\n",
    "\n",
    "\n",
    "    # attn (applied to every token embeding vector)\n",
    "    # query, key, vector\n",
    "    q_weights = parameters['transformer.h.'+ str(head_num) + '.attn.c_attn.weight'][:, :768].numpy()\n",
    "    k_weights = parameters['transformer.h.'+ str(head_num) + '.attn.c_attn.weight'][:, 768:1536].numpy()\n",
    "    v_weights = parameters['transformer.h.'+ str(head_num) + '.attn.c_attn.weight'][:, 1536:].numpy()\n",
    "\n",
    "    q_bias = parameters['transformer.h.'+ str(head_num) + '.attn.c_attn.bias'][:768].numpy()\n",
    "    k_bias = parameters['transformer.h.'+ str(head_num) + '.attn.c_attn.bias'][768:1536].numpy()\n",
    "    v_bias = parameters['transformer.h.'+ str(head_num) + '.attn.c_attn.bias'][1536:].numpy()\n",
    "\n",
    "    context_matrix = np.zeros_like(emb_norm1)\n",
    "    for i, tok_embed_vector in enumerate(emb_norm1): # each loop is a head\n",
    "        # qkv vectors (786,)\n",
    "        query = (tok_embed_vector @ q_weights.T) + q_bias\n",
    "        key = (tok_embed_vector @ k_weights.T) + k_bias\n",
    "        value = (tok_embed_vector @ v_weights.T) + v_bias\n",
    "\n",
    "        attn_score = query.reshape(query.shape[0], 1) @ key.reshape(1, key.shape[0]) # matrix\n",
    "        attn_score = attn_score / (key.shape[0])**(1/2)\n",
    "        attn_prob = np.zeros_like(attn_score) #(786, 786)\n",
    "        for j, row in enumerate(attn_score):\n",
    "            attn_prob[j] = softmax(row)\n",
    "\n",
    "        context_vec =  attn_prob @ value\n",
    "\n",
    "        weights = parameters['transformer.h.'+ str(head_num) + '.attn.c_proj.weight'].numpy()\n",
    "        bias = parameters['transformer.h.'+ str(head_num) + '.attn.c_proj.bias'].numpy()\n",
    "        context_vec_scaled = (context_vec @ weights) + bias # (768,)\n",
    "        context_matrix[i] = context_vec_scaled\n",
    "\n",
    "    print(context_matrix.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # ln_2 normalization\n",
    "    # weights = parameters['transformer.h.'+ str(head_num) + '.ln_2.weight']\n",
    "    # bias = parameters['transformer.h.'+ str(head_num) + '.ln_2.bias']\n",
    "    # emb_norm2 =  (_ * weights) + bias\n",
    "\n",
    "    # # mlp\n",
    "    # weights = parameters['transformer.h.'+ str(head_num) + '.ln_2.weight']\n",
    "    # bias = parameters['transformer.h.'+ str(head_num) + '.ln_2.bias']\n",
    "    # emb_mlp_l1 = (emb_norm2 @ weights.T) + bias\n",
    "\n",
    "    # weights = parameters['transformer.h.'+ str(head_num) + '.mlp.c_proj.weight']\n",
    "    # bias = parameters['transformer.h.'+ str(head_num) + '.mlp.c_proj.bias']\n",
    "    # emb_mlp_l2 = (emb_mlp_l1 @ weights.T) + bias\n",
    "    # return(emb_mlp_l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(prompt, parameters):\n",
    "    '''\n",
    "    prompt (str):\n",
    "    parameters(dict): dictionary maping names to tensors\n",
    "    '''\n",
    "    tok = gpt2_tokenizer.encode(prompt, return_tensors='np')\n",
    "    tok = tok.squeeze()\n",
    "\n",
    "     # word token embeddings\n",
    "    tok_emb = parameters['transformer.wte.weight'][tok,:].numpy()\n",
    "\n",
    "    # word position embeddings\n",
    "    sequence_length = tok.shape[0]\n",
    "    position_ids = np.arange(sequence_length) #indicies\n",
    "    position_emb = parameters['transformer.wpe.weight'][position_ids,:].numpy()\n",
    "    emb = tok_emb + position_emb\n",
    "\n",
    "    head_result = emb\n",
    "    for head_num in range(12):\n",
    "        head_result = decode_block(head_result, parameters, head_num)\n",
    "\n",
    "    # decoded = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 768)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mso its jsut a tensor. More you know\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[141], line 20\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(prompt, parameters)\u001b[0m\n\u001b[1;32m     18\u001b[0m head_result \u001b[38;5;241m=\u001b[39m emb\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m head_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12\u001b[39m):\n\u001b[0;32m---> 20\u001b[0m     head_result \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_num\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[140], line 27\u001b[0m, in \u001b[0;36mdecode_block\u001b[0;34m(emb, parameters, head_num)\u001b[0m\n\u001b[1;32m     25\u001b[0m weights \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(head_num) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.ln_1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     26\u001b[0m bias \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(head_num) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.ln_1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 27\u001b[0m emb_norm1 \u001b[38;5;241m=\u001b[39m  (\u001b[43memb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m) \u001b[38;5;241m+\u001b[39m bias \u001b[38;5;66;03m# (768,)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# attn (applied to every token embeding vector)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# query, key, vector\u001b[39;00m\n\u001b[1;32m     32\u001b[0m q_weights \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(head_num) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.attn.c_attn.weight\u001b[39m\u001b[38;5;124m'\u001b[39m][:, :\u001b[38;5;241m768\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "main('so its jsut a tensor. More you know', state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
