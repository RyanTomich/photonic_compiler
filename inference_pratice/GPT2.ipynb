{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using packages\n",
    "\n",
    "Differen gpt2 model options\n",
    "- gpt2: This is the \"small\" version of GPT-2. It has 124 million parameters.\n",
    "- gpt2-medium: This is the \"medium\" version of GPT-2. It has 355 million parameters.\n",
    "- gpt2-large: This is the \"large\" version of GPT-2. It has 774 million parameters.\n",
    "- gpt2-xl: This is the \"extra large\" version of GPT-2. It has 1.5 billion parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained('gpt2') # loading gpt2 from transformers library\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # loading gpt2 tokenizer from transformers library\n",
    "# print(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "music is a great way to get a sense of what's going on in your life.\n",
      "\n",
      "I think it's important to understand that you're not just talking about the music. You're talking about the people who are listening to it.\n",
      "\n",
      "I think that's a good thing. I think that's a good thing. I think that's a good thing. I think that's a good thing. I think that's a good thing. I think that's a good thing. I\n"
     ]
    }
   ],
   "source": [
    "input_text = \"A long time ago in a galaxy far far away ...\"\n",
    "input_text = \"music is\"\n",
    "input_ids = gpt2_tokenizer.encode(input_text, return_tensors='pt') # tokenize input\n",
    "output = gpt2.generate(input_ids, max_length=100) # run inference\n",
    "generated_text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True) # decode output tokens\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = gpt2.state_dict()\n",
    "parameters = {}\n",
    "for name, val in state_dict.items():\n",
    "    parameters[name] = val.numpy()\n",
    "\n",
    "\n",
    "for name, param in state_dict.items():\n",
    "    ans = param.numpy()\n",
    "    if 'h.' not in name: # each h.# refers to a transformer blocks\n",
    "        # print(f'{name}: {ans.shape}')\n",
    "        pass\n",
    "\n",
    "for i in range(12):\n",
    "    counter = 0\n",
    "    for name, param in state_dict.items():\n",
    "        ans = param.numpy()\n",
    "        if 'h.'+ str(i)+ '.' in name: # each h.# refers to a transformer block\n",
    "            # print(f'{name}: {ans.shape}')\n",
    "            counter +=1\n",
    "    # print(f'h.{i}: {counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "import random\n",
    "\n",
    "parameters = parameters\n",
    "\n",
    "def torch_to_numpy(tensor): # not nessessarry?\n",
    "    if tensor.is_cuda:\n",
    "        tensor = tensor.cpu()\n",
    "    numpy_array = tensor.numpy()\n",
    "    return numpy_array.copy()\n",
    "\n",
    "# TODO which softmax?\n",
    "def softmax(vec): # the, and\n",
    "    max_val = np.max(vec)\n",
    "    exp = np.exp(vec - max_val)\n",
    "    sum_exp = np.sum(exp)\n",
    "    norm_vec = exp/sum_exp\n",
    "    assert 0.975 < np.sum(norm_vec) < 1.025\n",
    "    return norm_vec\n",
    "\n",
    "def log_softmax(vec, epsilon=1e-12): # puncuation\n",
    "    max_val = np.max(vec)\n",
    "    exp = np.exp(vec - max_val)\n",
    "    log_sum_exp = max_val + np.log(np.sum(exp))\n",
    "    return vec - log_sum_exp\n",
    "\n",
    "def repo_softmax(vec): # error\n",
    "    x = vec - np.argmin(vec)\n",
    "    ex = np.exp(x)\n",
    "    return ex/ np.sum(ex)\n",
    "\n",
    "# activation functions\n",
    "def gelu(x):\n",
    "    # from https://github.com/openai/gpt-2.git\n",
    "    # x(np_array) Gausien error liner unit\n",
    "    return 0.5*x*(1+np.tanh(np.sqrt(2/np.pi)*(x+0.044715*np.power(x, 3))))\n",
    "\n",
    "def ReLU(x):\n",
    "    # x(np_array) clip negitive activation\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Transformer functions\n",
    "def embed(tok):\n",
    "    '''\n",
    "    creat embeding matrix (token, token embeding vector 768)\n",
    "    tok(np_array): 1d array of toek encodings\n",
    "    paramaters(dict): dictionary maping names to tensors\n",
    "    '''\n",
    "    # word token embeddings\n",
    "    tok_emb = parameters['transformer.wte.weight'][tok,:]\n",
    "\n",
    "    # word position embeddings\n",
    "    sequence_length = tok.shape[0]\n",
    "    position_ids = np.arange(sequence_length) #indicies\n",
    "    position_emb = parameters['transformer.wpe.weight'][position_ids,:]\n",
    "    assert tok_emb.shape == position_emb.shape\n",
    "    return tok_emb + position_emb\n",
    "\n",
    "def li_norm(x, gamma, beta, epsilon=1e-5):\n",
    "    '''\n",
    "    layer batch normalization\n",
    "    x(np_array): array to normalize\n",
    "    gamma(np_array): scailing paramater vector\n",
    "    beta(np_array): offset paramater vector\n",
    "    epsilon(float): div_0_error prevention\n",
    "    '''\n",
    "    u = np.mean(x, axis=-1, keepdims=True)\n",
    "    # s = np.var(x, axis=-1, keepdims=True)\n",
    "    s = np.mean(np.square(x-u))\n",
    "    x = (x - u) / np.sqrt(s + epsilon)\n",
    "    return x*gamma + beta\n",
    "\n",
    "def get_head_weights(head_tot, weights, bias):\n",
    "    '''\n",
    "    head_tot(int)\n",
    "    weights(np.matrix) (tok)\n",
    "    bias(np.vec)\n",
    "    return ittorator (tup): head_w, head_b\n",
    "    '''\n",
    "    head_width = int(weights.shape[1] / head_tot)\n",
    "    start = 0\n",
    "    end = head_width\n",
    "    for _ in range(head_tot):\n",
    "        yield (weights[:, start:end], bias[start:end])\n",
    "        start += head_width\n",
    "        end += head_width\n",
    "\n",
    "def split_head(head_param):\n",
    "    '''\n",
    "    head_param (tup): head_w, head_b\n",
    "    '''\n",
    "    weights, bias = head_param\n",
    "    width = int(head_param[1].shape[0] / 3)\n",
    "\n",
    "    start = 0\n",
    "    end = width\n",
    "    for _ in range(3):\n",
    "        yield (weights[:, start:end], bias[start:end])\n",
    "        start += width\n",
    "        end += width\n",
    "\n",
    "prev_QKV = {}\n",
    "\n",
    "def self_attn(emb, block_num):\n",
    "    '''\n",
    "    attention block. 12 heads per block\n",
    "    emb(np_matrix): (tokens, Embedding Size 768)\n",
    "    paramaters(dict): dictionary maping names to tensors\n",
    "    block_num: current head\n",
    "    '''\n",
    "    attn_weights = parameters['transformer.h.'+ str(block_num) + '.attn.c_attn.weight']\n",
    "    attn_bias = parameters['transformer.h.'+ str(block_num) + '.attn.c_attn.bias']\n",
    "\n",
    "    context_matrix = np.empty((emb.shape[0],0))\n",
    "    for head_num, head_param in enumerate(get_head_weights(12, attn_weights, attn_bias)):\n",
    "        QKV_gen = split_head(head_param)\n",
    "        query_w, query_b = next(QKV_gen)\n",
    "        key_w, key_b = next(QKV_gen)\n",
    "        value_w, value_b = next(QKV_gen)\n",
    "\n",
    "        if f'q_b{block_num}_h{head_num}' in prev_QKV:\n",
    "            new_emb = emb[-1]\n",
    "            np.vstack((prev_QKV[f'q_b{block_num}_h{head_num}'], new_emb @ query_w + query_b))\n",
    "            np.vstack((prev_QKV[f'k_b{block_num}_h{head_num}'], new_emb @ key_w + key_b))\n",
    "            np.vstack((prev_QKV[f'v_b{block_num}_h{head_num}'], new_emb @ value_w + value_b))\n",
    "            Q = prev_QKV[f'q_b{block_num}_h{head_num}']\n",
    "            K = prev_QKV[f'k_b{block_num}_h{head_num}']\n",
    "            V = prev_QKV[f'v_b{block_num}_h{head_num}']\n",
    "        else:\n",
    "            Q = np.apply_along_axis(lambda x: x + query_b, axis = 1, arr=emb @ query_w)\n",
    "            K = np.apply_along_axis(lambda x: x + key_b, axis = 1, arr=emb @ key_w)\n",
    "            V = np.apply_along_axis(lambda x: x + value_b, axis = 1, arr=emb @ value_w)\n",
    "\n",
    "            prev_QKV[f'q_b{block_num}_h{head_num}'] = Q\n",
    "            prev_QKV[f'k_b{block_num}_h{head_num}'] = K\n",
    "            prev_QKV[f'v_b{block_num}_h{head_num}'] = V\n",
    "\n",
    "        # Q = np.apply_along_axis(lambda x: x + query_b, axis = 1, arr=emb @ query_w)\n",
    "        # K = np.apply_along_axis(lambda x: x + key_b, axis = 1, arr=emb @ key_w)\n",
    "        # V = np.apply_along_axis(lambda x: x + value_b, axis = 1, arr=emb @ value_w)\n",
    "\n",
    "        attn_score = Q @ K.T\n",
    "\n",
    "        attn_score = attn_score / np.sqrt(emb.shape[1])\n",
    "\n",
    "        # future_mask\n",
    "        future_mask = np.full(attn_score.shape, 0.0)\n",
    "        future_mask[np.triu_indices_from(future_mask, k=1)] = float('-inf')\n",
    "\n",
    "        attn_score_mask = attn_score + future_mask\n",
    "\n",
    "        attn_score_norm = np.apply_along_axis(softmax, axis=1, arr=attn_score_mask)\n",
    "\n",
    "        context_matrix = np.hstack([context_matrix, attn_score_norm @ V])\n",
    "\n",
    "    return context_matrix\n",
    "\n",
    "def mlp(emb, block_num):\n",
    "    '''\n",
    "    2 layer multi layer perceptron with gelu activation\n",
    "    emb(np_matrix): (tokens, Embedding Size 768)\n",
    "    paramaters(dict): dictionary maping names to tensors\n",
    "    block_num: current head\n",
    "    '''\n",
    "    weights = parameters['transformer.h.'+ str(block_num) + '.mlp.c_fc.weight']\n",
    "    bias = parameters['transformer.h.'+ str(block_num) + '.mlp.c_fc.bias']\n",
    "    embl1 = (emb @ weights) + bias\n",
    "\n",
    "    embl1 = gelu(embl1)\n",
    "\n",
    "    weights = parameters['transformer.h.'+ str(block_num) + '.mlp.c_proj.weight']\n",
    "    bias = parameters['transformer.h.'+ str(block_num) + '.mlp.c_proj.bias']\n",
    "    return (embl1 @ weights) + bias\n",
    "\n",
    "def top_k(k, vec):\n",
    "    largest = heapq.nlargest(k, range(len(vec)), vec.take)\n",
    "    # print(gpt2_tokenizer.decode(largest, skip_special_tokens=True)) # see words its picking from.\n",
    "    probs = [vec[i] for i in largest]\n",
    "    return random.choices(largest, weights=probs, k=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = parameters\n",
    "\n",
    "def decode_block(emb, block_num):\n",
    "    '''\n",
    "    runs decode block with ln_1 -> attn -> ln_2 -> mlp\n",
    "    emb (np_array): (tokens, Embedding Size 768)\n",
    "    paramaters(dict): dictionary maping names to tensors\n",
    "    block_num: current head\n",
    "    '''\n",
    "    # ln_1 normalization\n",
    "    weights = parameters['transformer.h.'+ str(block_num) + '.ln_1.weight']\n",
    "    bias = parameters['transformer.h.'+ str(block_num) + '.ln_1.bias']\n",
    "    emb_norm1 = li_norm(emb, weights, bias)\n",
    "\n",
    "\n",
    "    context_matrix = self_attn(emb_norm1, block_num)\n",
    "    context_matrix += emb # Residual Connection\n",
    "\n",
    "    # ln_2 normalization\n",
    "    weights = parameters['transformer.h.'+ str(block_num) + '.ln_2.weight']\n",
    "    bias = parameters['transformer.h.'+ str(block_num) + '.ln_2.bias']\n",
    "    emb_norm2 = li_norm(context_matrix, weights, bias, epsilon=1e-5)\n",
    "\n",
    "    emb_mlp = mlp(emb_norm2, block_num)\n",
    "    emb_mlp += emb_norm2    # Residual Connection\n",
    "    return emb_mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_token(tok):\n",
    "    '''\n",
    "    Generates the next token in sequence\n",
    "    tok (np_array): 1D token encodigns\n",
    "    parameters(dict): dictionary maping names to tensors\n",
    "    '''\n",
    "    emb = embed(tok) #(tokens, Embedding Size 768)\n",
    "\n",
    "    block_result = emb\n",
    "    for block in range(12): # for every decode block\n",
    "        block_result = decode_block(block_result, block) # (tokens, Embedding Size 768)\n",
    "\n",
    "    # ln_f\n",
    "    weights = parameters['transformer.ln_f.weight']\n",
    "    bias = parameters['transformer.ln_f.bias']\n",
    "    head_norm = li_norm(block_result, weights, bias)\n",
    "\n",
    "    # lm_head\n",
    "    weights = parameters['lm_head.weight'] # (50257, 768)\n",
    "    logit_matrix = head_norm @ weights.T\n",
    "\n",
    "    # apply softmax to last words logit\n",
    "    last_logit_distrabution = softmax(logit_matrix[-1])\n",
    "    return top_k(5, last_logit_distrabution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  86  993   83  466  345  892   11 3612 3235   72]\n",
      "waht do you think, thinking machiene? the the the the the the you the the the\n"
     ]
    }
   ],
   "source": [
    "def main(prompt, max_token_len = 1024):\n",
    "    '''\n",
    "    creates generation feedback loop\n",
    "    prompt(srt)\n",
    "    start_dict(dict): name: paramaters\n",
    "    '''\n",
    "    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "    prompt_tok_len = len(gpt2_tokenizer.encode(prompt, return_tensors='np').squeeze())\n",
    "    tok = gpt2_tokenizer.encode(prompt, return_tensors='np', padding='max_length', truncation=True, max_length=max_token_len)\n",
    "    tok = tok.squeeze()\n",
    "\n",
    "    for i in range(10):\n",
    "        if prompt_tok_len < max_token_len:\n",
    "            new_tok = next_token(tok)\n",
    "            tok[prompt_tok_len] = new_tok\n",
    "            prompt_tok_len += 1\n",
    "\n",
    "\n",
    "    print(tok[:10])\n",
    "    token_decoded = gpt2_tokenizer.decode(tok, skip_special_tokens=True)\n",
    "    return token_decoded\n",
    "\n",
    "print(main('waht do you think, thinking machiene?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
