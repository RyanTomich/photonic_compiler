{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using packages\n",
    "\n",
    "Differen gpt2 model options\n",
    "- gpt2: This is the \"small\" version of GPT-2. It has 124 million parameters.\n",
    "- gpt2-medium: This is the \"medium\" version of GPT-2. It has 355 million parameters.\n",
    "- gpt2-large: This is the \"large\" version of GPT-2. It has 774 million parameters.\n",
    "- gpt2-xl: This is the \"extra large\" version of GPT-2. It has 1.5 billion parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjtomich/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained('gpt2') # loading gpt2 from transformers library\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # loading gpt2 tokenizer from transformers library\n",
    "print(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A long time ago in a galaxy far far away...\n",
      "\n",
      "The first human-made planet was discovered in the early 1960s by a team of astronomers from the University of California, Berkeley.\n",
      "\n",
      "The discovery of the first human-made planet was made by a team of astronomers from the University of California, Berkeley.\n",
      "\n",
      "The first human-made planet was discovered in the early 1960s by a team of astronomers from the University of California, Berkeley.\n",
      "\n",
      "The first human-made\n"
     ]
    }
   ],
   "source": [
    "input_text = \"A long time ago in a galaxy far far away ...\"\n",
    "input_ids = gpt2_tokenizer.encode(input_text, return_tensors='pt') # tokenize input\n",
    "output = gpt2.generate(input_ids, max_length=100) # run inference\n",
    "generated_text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True) # decode output tokens\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0.ln_1.weight: (768,)\n",
      "transformer.h.0.ln_1.bias: (768,)\n",
      "transformer.h.0.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.0.attn.c_attn.bias: (2304,)\n",
      "transformer.h.0.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.0.attn.c_proj.bias: (768,)\n",
      "transformer.h.0.ln_2.weight: (768,)\n",
      "transformer.h.0.ln_2.bias: (768,)\n",
      "transformer.h.0.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.0.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.0.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.0.mlp.c_proj.bias: (768,)\n",
      "transformer.h.1.ln_1.weight: (768,)\n",
      "transformer.h.1.ln_1.bias: (768,)\n",
      "transformer.h.1.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.1.attn.c_attn.bias: (2304,)\n",
      "transformer.h.1.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.1.attn.c_proj.bias: (768,)\n",
      "transformer.h.1.ln_2.weight: (768,)\n",
      "transformer.h.1.ln_2.bias: (768,)\n",
      "transformer.h.1.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.1.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.1.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.1.mlp.c_proj.bias: (768,)\n",
      "transformer.h.2.ln_1.weight: (768,)\n",
      "transformer.h.2.ln_1.bias: (768,)\n",
      "transformer.h.2.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.2.attn.c_attn.bias: (2304,)\n",
      "transformer.h.2.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.2.attn.c_proj.bias: (768,)\n",
      "transformer.h.2.ln_2.weight: (768,)\n",
      "transformer.h.2.ln_2.bias: (768,)\n",
      "transformer.h.2.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.2.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.2.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.2.mlp.c_proj.bias: (768,)\n",
      "transformer.h.3.ln_1.weight: (768,)\n",
      "transformer.h.3.ln_1.bias: (768,)\n",
      "transformer.h.3.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.3.attn.c_attn.bias: (2304,)\n",
      "transformer.h.3.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.3.attn.c_proj.bias: (768,)\n",
      "transformer.h.3.ln_2.weight: (768,)\n",
      "transformer.h.3.ln_2.bias: (768,)\n",
      "transformer.h.3.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.3.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.3.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.3.mlp.c_proj.bias: (768,)\n",
      "transformer.h.4.ln_1.weight: (768,)\n",
      "transformer.h.4.ln_1.bias: (768,)\n",
      "transformer.h.4.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.4.attn.c_attn.bias: (2304,)\n",
      "transformer.h.4.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.4.attn.c_proj.bias: (768,)\n",
      "transformer.h.4.ln_2.weight: (768,)\n",
      "transformer.h.4.ln_2.bias: (768,)\n",
      "transformer.h.4.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.4.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.4.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.4.mlp.c_proj.bias: (768,)\n",
      "transformer.h.5.ln_1.weight: (768,)\n",
      "transformer.h.5.ln_1.bias: (768,)\n",
      "transformer.h.5.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.5.attn.c_attn.bias: (2304,)\n",
      "transformer.h.5.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.5.attn.c_proj.bias: (768,)\n",
      "transformer.h.5.ln_2.weight: (768,)\n",
      "transformer.h.5.ln_2.bias: (768,)\n",
      "transformer.h.5.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.5.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.5.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.5.mlp.c_proj.bias: (768,)\n",
      "transformer.h.6.ln_1.weight: (768,)\n",
      "transformer.h.6.ln_1.bias: (768,)\n",
      "transformer.h.6.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.6.attn.c_attn.bias: (2304,)\n",
      "transformer.h.6.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.6.attn.c_proj.bias: (768,)\n",
      "transformer.h.6.ln_2.weight: (768,)\n",
      "transformer.h.6.ln_2.bias: (768,)\n",
      "transformer.h.6.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.6.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.6.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.6.mlp.c_proj.bias: (768,)\n",
      "transformer.h.7.ln_1.weight: (768,)\n",
      "transformer.h.7.ln_1.bias: (768,)\n",
      "transformer.h.7.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.7.attn.c_attn.bias: (2304,)\n",
      "transformer.h.7.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.7.attn.c_proj.bias: (768,)\n",
      "transformer.h.7.ln_2.weight: (768,)\n",
      "transformer.h.7.ln_2.bias: (768,)\n",
      "transformer.h.7.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.7.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.7.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.7.mlp.c_proj.bias: (768,)\n",
      "transformer.h.8.ln_1.weight: (768,)\n",
      "transformer.h.8.ln_1.bias: (768,)\n",
      "transformer.h.8.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.8.attn.c_attn.bias: (2304,)\n",
      "transformer.h.8.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.8.attn.c_proj.bias: (768,)\n",
      "transformer.h.8.ln_2.weight: (768,)\n",
      "transformer.h.8.ln_2.bias: (768,)\n",
      "transformer.h.8.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.8.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.8.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.8.mlp.c_proj.bias: (768,)\n",
      "transformer.h.9.ln_1.weight: (768,)\n",
      "transformer.h.9.ln_1.bias: (768,)\n",
      "transformer.h.9.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.9.attn.c_attn.bias: (2304,)\n",
      "transformer.h.9.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.9.attn.c_proj.bias: (768,)\n",
      "transformer.h.9.ln_2.weight: (768,)\n",
      "transformer.h.9.ln_2.bias: (768,)\n",
      "transformer.h.9.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.9.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.9.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.9.mlp.c_proj.bias: (768,)\n",
      "transformer.h.10.ln_1.weight: (768,)\n",
      "transformer.h.10.ln_1.bias: (768,)\n",
      "transformer.h.10.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.10.attn.c_attn.bias: (2304,)\n",
      "transformer.h.10.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.10.attn.c_proj.bias: (768,)\n",
      "transformer.h.10.ln_2.weight: (768,)\n",
      "transformer.h.10.ln_2.bias: (768,)\n",
      "transformer.h.10.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.10.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.10.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.10.mlp.c_proj.bias: (768,)\n",
      "transformer.h.11.ln_1.weight: (768,)\n",
      "transformer.h.11.ln_1.bias: (768,)\n",
      "transformer.h.11.attn.c_attn.weight: (768, 2304)\n",
      "transformer.h.11.attn.c_attn.bias: (2304,)\n",
      "transformer.h.11.attn.c_proj.weight: (768, 768)\n",
      "transformer.h.11.attn.c_proj.bias: (768,)\n",
      "transformer.h.11.ln_2.weight: (768,)\n",
      "transformer.h.11.ln_2.bias: (768,)\n",
      "transformer.h.11.mlp.c_fc.weight: (768, 3072)\n",
      "transformer.h.11.mlp.c_fc.bias: (3072,)\n",
      "transformer.h.11.mlp.c_proj.weight: (3072, 768)\n",
      "transformer.h.11.mlp.c_proj.bias: (768,)\n"
     ]
    }
   ],
   "source": [
    "state_dict = gpt2.state_dict()\n",
    "parameters = {}\n",
    "for name, val in state_dict.items():\n",
    "    parameters[name] = val.numpy()\n",
    "\n",
    "\n",
    "for name, param in state_dict.items():\n",
    "    ans = param.numpy()\n",
    "    if 'h.' not in name: # each h.# refers to a transformer blocks\n",
    "        # print(f'{name}: {ans.shape}')\n",
    "        pass\n",
    "\n",
    "for i in range(12):\n",
    "    counter = 0\n",
    "    for name, param in state_dict.items():\n",
    "        ans = param.numpy()\n",
    "        if 'h.'+ str(i)+ '.' in name: # each h.# refers to a transformer block\n",
    "            print(f'{name}: {ans.shape}')\n",
    "            counter +=1\n",
    "    # print(f'h.{i}: {counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "parameters = parameters\n",
    "\n",
    "def torch_to_numpy(tensor): # not nessessarry?\n",
    "    if tensor.is_cuda:\n",
    "        tensor = tensor.cpu()\n",
    "    numpy_array = tensor.numpy()\n",
    "    return numpy_array.copy()\n",
    "\n",
    "# TODO which softmax?\n",
    "def softmax(vec): # the, and\n",
    "    max_val = np.max(vec)\n",
    "    exp = np.exp(vec - max_val)\n",
    "    sum_exp = np.sum(exp)\n",
    "    return exp/sum_exp\n",
    "\n",
    "def log_softmax(vec, epsilon=1e-12): # puncuation\n",
    "    max_val = np.max(vec)\n",
    "    exp = np.exp(vec - max_val)\n",
    "    log_sum_exp = max_val + np.log(np.sum(exp))\n",
    "    return vec - log_sum_exp\n",
    "\n",
    "def repo_softmax(vec): # error\n",
    "    x = vec - np.argmin(vec)\n",
    "    ex = np.exp(x)\n",
    "    return ex/ np.sum(ex)\n",
    "\n",
    "# activation functions\n",
    "def gelu(x):\n",
    "    # from https://github.com/openai/gpt-2.git\n",
    "    # x(np_array) Gausien error liner unit\n",
    "    return 0.5*x*(1+np.tanh(np.sqrt(2/np.pi)*(x+0.044715*np.power(x, 3))))\n",
    "\n",
    "def ReLU(x):\n",
    "    # x(np_array) clip negitive activation\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Transformer functions\n",
    "def embed(tok):\n",
    "    '''\n",
    "    creat embeding matrix (token, token embeding vector 768)\n",
    "    tok(np_array): 1d array of toek encodings\n",
    "    paramaters(dict): dictionary maping names to tensors\n",
    "    '''\n",
    "    # word token embeddings\n",
    "    tok_emb = parameters['transformer.wte.weight'][tok,:]\n",
    "\n",
    "    # word position embeddings\n",
    "    sequence_length = tok.shape[0]\n",
    "    position_ids = np.arange(sequence_length) #indicies\n",
    "    print(position_ids.shape)\n",
    "    position_emb = parameters['transformer.wpe.weight'][position_ids,:]\n",
    "    return tok_emb + position_emb\n",
    "\n",
    "def li_norm(x, gamma, beta, epsilon=1e-5):\n",
    "    '''\n",
    "    layer batch normalization\n",
    "    x(np_array): array to normalize\n",
    "    gamma(np_array): scailing paramater vector\n",
    "    beta(np_array): offset paramater vector\n",
    "    epsilon(float): div_0_error prevention\n",
    "    '''\n",
    "    u = np.mean(x, axis=-1, keepdims=True)\n",
    "    s = np.var(x, axis=-1, keepdims=True)\n",
    "    # s = np.mean(np.square(x-u))\n",
    "    x = (x - u) / np.sqrt(s + epsilon)\n",
    "    return x*gamma + beta\n",
    "\n",
    "def get_head_weights(head_tot, weights, bias):\n",
    "    '''\n",
    "    head_tot(int)\n",
    "    weights(np.matrix) (tok)\n",
    "    bias(np.vec)\n",
    "    return ittorator (tup): head_w, head_b\n",
    "    '''\n",
    "    head_width = int(weights.shape[1] / head_tot)\n",
    "    start = 0\n",
    "    end = head_width\n",
    "    for _ in range(head_tot):\n",
    "        yield (weights[:, start:end], bias[start:end])\n",
    "        start += head_width\n",
    "        end += head_width\n",
    "\n",
    "def split_head(head_param):\n",
    "    '''\n",
    "    head_param (tup): head_w, head_b\n",
    "    '''\n",
    "    weights, bias = head_param\n",
    "    width = int(head_param[1].shape[0] / 3)\n",
    "\n",
    "    start = 0\n",
    "    end = width\n",
    "    for _ in range(3):\n",
    "        yield (weights[:, start:end], bias[start:end])\n",
    "        start += width\n",
    "        end += width\n",
    "\n",
    "def self_attn(emb, block_num):\n",
    "    '''\n",
    "    attention block. 12 heads per block\n",
    "    emb(np_matrix): (tokens, Embedding Size 768)\n",
    "    paramaters(dict): dictionary maping names to tensors\n",
    "    block_num: current head\n",
    "    '''\n",
    "    attn_weights = parameters['transformer.h.'+ str(block_num) + '.attn.c_attn.weight']\n",
    "    attn_bias = parameters['transformer.h.'+ str(block_num) + '.attn.c_attn.bias']\n",
    "\n",
    "    context_matrix = np.empty((emb.shape[0],0))\n",
    "    for head_param in get_head_weights(12, attn_weights, attn_bias):\n",
    "        QKV_gen = split_head(head_param)\n",
    "        query_w, query_b = next(QKV_gen)\n",
    "        key_w, key_b = next(QKV_gen)\n",
    "        value_w, value_b = next(QKV_gen)\n",
    "\n",
    "        Q = np.apply_along_axis(lambda x: x + query_b, axis = 1, arr=emb @ query_w)\n",
    "        K = np.apply_along_axis(lambda x: x + key_b, axis = 1, arr=emb @ key_w)\n",
    "        V = np.apply_along_axis(lambda x: x + value_b, axis = 1, arr=emb @ value_w)\n",
    "\n",
    "        attn_score = Q @ K.T\n",
    "\n",
    "        attn_score = attn_score / np.sqrt(emb.shape[1])\n",
    "\n",
    "        attn_score_norm = np.apply_along_axis(softmax, axis=1, arr=attn_score_mask)\n",
    "\n",
    "        context_matrix = np.hstack([context_matrix, attn_score_norm @ V])\n",
    "\n",
    "    return context_matrix\n",
    "\n",
    "\n",
    "def mlp(emb, block_num):\n",
    "    '''\n",
    "    2 layer multi layer perceptron with gelu activation\n",
    "    emb(np_matrix): (tokens, Embedding Size 768)\n",
    "    paramaters(dict): dictionary maping names to tensors\n",
    "    block_num: current head\n",
    "    '''\n",
    "    weights = parameters['transformer.h.'+ str(block_num) + '.mlp.c_fc.weight']\n",
    "    bias = parameters['transformer.h.'+ str(block_num) + '.mlp.c_fc.bias']\n",
    "    embl1 = (emb @ weights) + bias\n",
    "\n",
    "    embl1 = gelu(embl1)\n",
    "\n",
    "    weights = parameters['transformer.h.'+ str(block_num) + '.mlp.c_proj.weight']\n",
    "    bias = parameters['transformer.h.'+ str(block_num) + '.mlp.c_proj.bias']\n",
    "    return (embl1 @ weights) + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = parameters\n",
    "\n",
    "def decode_block(emb, block_num):\n",
    "    '''\n",
    "    runs decode block with ln_1 -> attn -> ln_2 -> mlp\n",
    "    emb (np_array): (tokens, Embedding Size 768)\n",
    "    paramaters(dict): dictionary maping names to tensors\n",
    "    block_num: current head\n",
    "    '''\n",
    "    # ln_1 normalization\n",
    "    weights = parameters['transformer.h.'+ str(block_num) + '.ln_1.weight']\n",
    "    bias = parameters['transformer.h.'+ str(block_num) + '.ln_1.bias']\n",
    "    emb_norm1 = li_norm(emb, weights, bias)\n",
    "\n",
    "\n",
    "    context_matrix = self_attn(emb_norm1, block_num)\n",
    "    context_matrix += emb # Residual Connection\n",
    "\n",
    "    # ln_2 normalization\n",
    "    weights = parameters['transformer.h.'+ str(block_num) + '.ln_2.weight']\n",
    "    bias = parameters['transformer.h.'+ str(block_num) + '.ln_2.bias']\n",
    "    emb_norm2 = li_norm(context_matrix, weights, bias, epsilon=1e-5)\n",
    "\n",
    "    emb_mlp = mlp(emb_norm2, block_num)\n",
    "    emb_mlp += emb_norm2    # Residual Connection\n",
    "    return emb_mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_token(tok):\n",
    "    '''\n",
    "    Generates the next token in sequence\n",
    "    tok (np_array): 1D token encodigns\n",
    "    parameters(dict): dictionary maping names to tensors\n",
    "    '''\n",
    "    emb = embed(tok) #(tokens, Embedding Size 768)\n",
    "\n",
    "    block_result = emb\n",
    "    for block in range(12): # for every decode block\n",
    "        block_result = decode_block(block_result, block) # (tokens, Embedding Size 768)\n",
    "\n",
    "    # ln_f\n",
    "    weights = parameters['transformer.ln_f.weight']\n",
    "    bias = parameters['transformer.ln_f.bias']\n",
    "    head_norm = li_norm(block_result, weights, bias)\n",
    "\n",
    "    # lm_head\n",
    "    weights = parameters['lm_head.weight'] # (50257, 768)\n",
    "    logit_matrix = head_norm @ weights.T\n",
    "\n",
    "    # apply softmax to last words logit\n",
    "    last_logit_distrabution = softmax(logit_matrix[-1])\n",
    "    next_token = np.argmax(last_logit_distrabution)\n",
    "    return next_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1169  3807   373   220 50256 50256 50256 50256 50256 50256]\n",
      "(1024,)\n",
      "(1023,)\n",
      "(1022,)\n",
      "[ 1169  3807   373   220   262   262   262 50256 50256 50256]\n",
      "the movie was  the the the\n"
     ]
    }
   ],
   "source": [
    "def main(prompt, max_token_len = 1024):\n",
    "    '''\n",
    "    creates generation feedback loop\n",
    "    prompt(srt)\n",
    "    start_dict(dict): name: paramaters\n",
    "    '''\n",
    "    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "    prompt_tok_len = len(gpt2_tokenizer.encode(prompt, return_tensors='np').squeeze())\n",
    "    tok = gpt2_tokenizer.encode(prompt, return_tensors='np', padding='max_length', truncation=True, max_length=max_token_len)\n",
    "    tok = tok.squeeze()\n",
    "\n",
    "    for i in range(3):\n",
    "        if prompt_tok_len < max_token_len:\n",
    "            new_tok = next_token(tok)\n",
    "            tok[prompt_tok_len] = new_tok\n",
    "            tok = tok[:-1]\n",
    "            prompt_tok_len += 1\n",
    "\n",
    "\n",
    "    print(tok[:10])\n",
    "    token_decoded = gpt2_tokenizer.decode(tok, skip_special_tokens=True)\n",
    "    return token_decoded\n",
    "\n",
    "print(main('the movie was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
